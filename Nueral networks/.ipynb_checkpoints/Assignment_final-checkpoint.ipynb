{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (m,) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5  ...      0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8  ...      1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22093d274e0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2209392b860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1000\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the **`sigmoid`** function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(Z) = (array([[0.5       , 0.73105858],\n",
      "       [0.88079708, 0.95257413],\n",
      "       [0.98201379, 0.99330715],\n",
      "       [0.99752738, 0.99908895]]), array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.arange(8).reshape(4,2)\n",
    "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`relu`** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(Z) = (array([[ 1,  3],\n",
      "       [ 0,  0],\n",
      "       [ 0,  7],\n",
      "       [ 9, 18]]), array([[ 1,  3],\n",
      "       [-1, -4],\n",
      "       [-5,  7],\n",
      "       [ 9, 18]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
    "print (\"relu(Z) = \" + str(relu(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`softmax`** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[11,19,10], [12, 21, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n",
      " [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n",
      "[[11 19 10]\n",
      " [12 21 23]]\n"
     ]
    }
   ],
   "source": [
    "#Z = np.array(np.arange(30)).reshape(10,3)\n",
    "H, softmax_memory = softmax(Z)\n",
    "print(H)\n",
    "print(softmax_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **`initialize_parameters`** function initializes the parameters for all the layers in one `for` loop. \n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
    "\n",
    "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
      "   0.00545832]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
      " [-0.28074571 -0.13967752  0.02641189]\n",
      " [ 0.10925169  0.06646016  0.08565535]\n",
      " [-0.11058228  0.03715795  0.13440124]\n",
      " [-0.16421272 -0.1153127   0.02013163]\n",
      " [ 0.13985659  0.07228733 -0.10717236]\n",
      " [-0.05673344 -0.03663499 -0.15460347]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
    "\n",
    "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **`sigmoid`**, **`relu`** and **`softmax`** on **Z**.\n",
    "\n",
    "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = W.dot(H_prev)+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = W.dot(H_prev)+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z = W.dot(H_prev)+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
       "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    array([[1.        , 1.        , 1.        , 1.        , 1.        ],<br>\n",
    "      [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],<br>\n",
    "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **`relu`**. The last layer having a different activation **`softmax`** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev =  H\n",
    "        H, memory = layer_forward(H_prev,parameters['W' + str(l)],parameters['b' + str(l)])\n",
    "       \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H,parameters['W' + str(L)],parameters['b' + str(L)],activation = 'softmax')\n",
    "    \n",
    "    memories.append(memory)\n",
    "    print(HL.shape)\n",
    "    #assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "(10, 10)\n",
      "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n",
      " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n",
      " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n",
      " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n",
      " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n",
      " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n",
      " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n",
      " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n",
      " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n",
      " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "#print(len(parameters))\n",
    "#print(parameters.keys())\n",
    "#print(layer_forward(x_sample,parameters['W' + str(1)],parameters['b' + str(1)]))\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "(784, 10)<br>\n",
    "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]<br>\n",
    " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]<br>\n",
    " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]<br>\n",
    " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]<br>\n",
    " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]<br>\n",
    " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]<br>\n",
    " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]<br>\n",
    " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]<br>\n",
    " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]<br>\n",
    " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### compute_loss\n",
    "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **`compute_loss`** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = -1.0/m * np.sum(Y*np.log(HL)+(1-Y)*(np.log(1-HL)))\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "7.950636624741581\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    \n",
    "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]<br>\n",
    " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]<br>\n",
    " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]<br>\n",
    " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]<br>\n",
    " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]<br>\n",
    " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]<br>\n",
    " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]<br>\n",
    " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]<br>\n",
    " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]<br>\n",
    " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]<br>\n",
    "[[0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 1.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [1. 0. 1. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 1. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 1. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]]<br>\n",
    "0.8964600261334037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
    "\n",
    "### sigmoid-backward\n",
    "You might remember that we had created **`sigmoid`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward\n",
    "You might remember that we had created **`relu`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n",
    "\n",
    "**`layer_backward`** is a complimentary function of **`layer_forward`**. Like **`layer_forward`** calculates **H** using **W**, **H_prev** and **b**, **`layer_backward`** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **`sigmoid_backward`** and **`relu_backward`** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW=(1./m) * dZ.dot(H_prev.T)\n",
    "        db = (1./m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "        dH_prev = W.T.dot(dZ)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, sigmoid_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW =(1./m) * dZ.dot(H_prev.T)\n",
    "        db = (1./m) * dZ\n",
    "        dH_prev =dW.T.dot(dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "dH_prev is <br>\n",
    " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]<br>\n",
    " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]<br>\n",
    "dW is <br>\n",
    " [[1.67565336 1.56891359]<br>\n",
    " [1.39137819 1.4143854 ]<br>\n",
    " [1.3597389  1.43013369]]<br>\n",
    "db is <br>\n",
    " [[0.37345476]<br>\n",
    " [0.34414727]<br>\n",
    " [0.29074635]]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n",
    "\n",
    "**`L_layer_backward`** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **`layer_backward`** here in the loop with the activation function as **`relu`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L-1)] = W.T.dot(dZ)\n",
    "    gradients[\"dW\" + str(L)] = (1./m) * dZ.dot(H_prev.T)\n",
    "    gradients[\"db\" + str(L)] = (1./m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "    \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)],current_memory, activation = 'relu')\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "dW3 is \n",
      " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
      "   0.        ]\n",
      " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
      "   0.        ]\n",
      " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
      "   0.        ]\n",
      " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
      "   0.        ]\n",
      " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
      "   0.        ]\n",
      " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
      "   0.        ]\n",
      " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
      "   0.        ]\n",
      " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
      "   0.        ]\n",
      " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
      "   0.        ]\n",
      " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
      "   0.        ]]\n",
      "db3 is \n",
      " [[ 0.10031756]\n",
      " [ 0.00460183]\n",
      " [-0.00142942]\n",
      " [-0.0997827 ]\n",
      " [ 0.09872663]\n",
      " [ 0.00536378]\n",
      " [-0.10124784]\n",
      " [-0.00191121]\n",
      " [-0.00359044]\n",
      " [-0.00104818]]\n",
      "dW2 is \n",
      " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]\n",
      " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]\n",
      " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]\n",
      " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]\n",
      " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]\n",
      " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "db2 is \n",
      " [[ 0.013979  ]\n",
      " [-0.01329383]\n",
      " [ 0.01275707]\n",
      " [-0.01052957]\n",
      " [ 0.03179224]\n",
      " [-0.00039877]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "y_sample = train_set_y[:, 10:20]\n",
    "\n",
    "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = L_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "\n",
    "dW3 is <br>\n",
    " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863  0.        ]<br>\n",
    " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533   0.        ]<br>\n",
    " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996   0.        ]<br>\n",
    " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381   0.        ]<br>\n",
    " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682   0.        ]<br>\n",
    " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441   0.        ]<br>\n",
    " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034   0.        ]<br>\n",
    " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535   0.        ]<br>\n",
    " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264   0.        ]<br>\n",
    " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965    0.        ]]<br>\n",
    "db3 is <br>\n",
    " [[ 0.10031756]<br>\n",
    " [ 0.00460183]<br>\n",
    " [-0.00142942]<br>\n",
    " [-0.0997827 ]<br>\n",
    " [ 0.09872663]<br>\n",
    " [ 0.00536378]<br>\n",
    " [-0.10124784]<br>\n",
    " [-0.00191121]<br>\n",
    " [-0.00359044]<br>\n",
    " [-0.00104818]]<br>\n",
    "dW2 is <br>\n",
    " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]<br>\n",
    " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]<br>\n",
    " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]<br>\n",
    " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]<br>\n",
    " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]<br>\n",
    " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]<br>\n",
    " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]<br>\n",
    "db2 is <br>\n",
    " [[ 0.013979  ]<br>\n",
    " [-0.01329383]<br>\n",
    " [ 0.01275707]<br>\n",
    " [-0.01052957]<br>\n",
    " [ 0.03179224]<br>\n",
    " [-0.00039877]<br>\n",
    " [ 0.        ]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate* gradients[\"dW\" + str(l + 1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate* gradients[\"db\" + str(l + 1)])\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### L_layer_model\n",
    "\n",
    "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have written for feedforward, computing the loss, backpropagation and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 5000)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:5000]\n",
    "train_set_y_new = train_set_y[:,0:5000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "Loss after iteration 0: 3.387393\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 100: 3.055395\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 200: 2.759272\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 300: 2.425031\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 400: 2.096039\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 500: 1.816691\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 600: 1.598000\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 700: 1.430997\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 800: 1.302446\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 900: 1.201585\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1000: 1.120573\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1100: 1.054110\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1200: 0.998611\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1300: 0.951556\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1400: 0.911071\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1500: 0.875873\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1600: 0.844950\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1700: 0.817542\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1800: 0.793049\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "Loss after iteration 1900: 0.771000\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n",
      "(10, 5000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22094d47b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuray we get on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "Accuracy: 0.8774000000000002\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "Accuracy: 0.8674000000000002\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ~87%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
    "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22094dbde48>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22094f9b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3474\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
