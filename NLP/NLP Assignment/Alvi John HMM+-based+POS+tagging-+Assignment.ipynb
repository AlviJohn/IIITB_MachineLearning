{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import re\n",
    "import pprint,time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import requests\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "print(nltk_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into train and test\n",
    "random.seed(1000)\n",
    "train_set, val_set = train_test_split(nltk_data,test_size=0.05)\n",
    "print(len(train_set))\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95638"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words=[tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "V=set(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'DET', 'VERB', 'X', 'PRON', 'CONJ', 'PRT', '.', 'ADJ', 'NOUN', 'ADV', 'NUM', 'ADP'}\n"
     ]
    }
   ],
   "source": [
    "# number of tags\n",
    "tags=[pair[1] for pair in train_tagged_words]\n",
    "T = set(tags)\n",
    "print(len(T))\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4262, 6078)\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "print(t2_given_t1(t2='NOUN', t1='ADJ'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DET</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "      <th>PRON</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>.</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>0.045804</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.017793</td>\n",
       "      <td>0.204616</td>\n",
       "      <td>0.638134</td>\n",
       "      <td>0.012743</td>\n",
       "      <td>0.022121</td>\n",
       "      <td>0.009377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.135204</td>\n",
       "      <td>0.168540</td>\n",
       "      <td>0.218234</td>\n",
       "      <td>0.035972</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.031553</td>\n",
       "      <td>0.034809</td>\n",
       "      <td>0.066052</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.081014</td>\n",
       "      <td>0.022405</td>\n",
       "      <td>0.091247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.055062</td>\n",
       "      <td>0.205173</td>\n",
       "      <td>0.074738</td>\n",
       "      <td>0.055379</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.183910</td>\n",
       "      <td>0.163440</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.061726</td>\n",
       "      <td>0.026341</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.144240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.009560</td>\n",
       "      <td>0.483365</td>\n",
       "      <td>0.092161</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.005354</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.040918</td>\n",
       "      <td>0.074952</td>\n",
       "      <td>0.208031</td>\n",
       "      <td>0.034034</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.023327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.122077</td>\n",
       "      <td>0.157156</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.058466</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>0.034612</td>\n",
       "      <td>0.117867</td>\n",
       "      <td>0.344715</td>\n",
       "      <td>0.056127</td>\n",
       "      <td>0.041628</td>\n",
       "      <td>0.053789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.101142</td>\n",
       "      <td>0.401631</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.018597</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.041436</td>\n",
       "      <td>0.084829</td>\n",
       "      <td>0.246982</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.057096</td>\n",
       "      <td>0.020881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.174383</td>\n",
       "      <td>0.088453</td>\n",
       "      <td>0.027112</td>\n",
       "      <td>0.066294</td>\n",
       "      <td>0.057827</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.093317</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>0.220141</td>\n",
       "      <td>0.053594</td>\n",
       "      <td>0.080886</td>\n",
       "      <td>0.091425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.064988</td>\n",
       "      <td>0.066798</td>\n",
       "      <td>0.701218</td>\n",
       "      <td>0.004607</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.076505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.013160</td>\n",
       "      <td>0.147536</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.042477</td>\n",
       "      <td>0.044195</td>\n",
       "      <td>0.239801</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.263525</td>\n",
       "      <td>0.016998</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0.177438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.068988</td>\n",
       "      <td>0.346932</td>\n",
       "      <td>0.022886</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.136982</td>\n",
       "      <td>0.126036</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.079270</td>\n",
       "      <td>0.030514</td>\n",
       "      <td>0.120066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.211573</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>0.116024</td>\n",
       "      <td>0.033828</td>\n",
       "      <td>0.350742</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.187240</td>\n",
       "      <td>0.034718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.035394</td>\n",
       "      <td>0.069510</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.039659</td>\n",
       "      <td>0.106290</td>\n",
       "      <td>0.320789</td>\n",
       "      <td>0.013113</td>\n",
       "      <td>0.063220</td>\n",
       "      <td>0.017377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DET      VERB         X      PRON      CONJ       PRT         .  \\\n",
       "DET   0.005410  0.039793  0.045804  0.003486  0.000481  0.000240  0.017793   \n",
       "VERB  0.135204  0.168540  0.218234  0.035972  0.005117  0.031553  0.034809   \n",
       "X     0.055062  0.205173  0.074738  0.055379  0.010632  0.183910  0.163440   \n",
       "PRON  0.009560  0.483365  0.092161  0.008031  0.005354  0.013002  0.040918   \n",
       "CONJ  0.122077  0.157156  0.008419  0.058466  0.000468  0.004677  0.034612   \n",
       "PRT   0.101142  0.401631  0.013051  0.018597  0.002284  0.001958  0.041436   \n",
       ".     0.174383  0.088453  0.027112  0.066294  0.057827  0.002252  0.093317   \n",
       "ADJ   0.004936  0.011681  0.020895  0.000494  0.016453  0.010201  0.064988   \n",
       "NOUN  0.013160  0.147536  0.029025  0.004752  0.042477  0.044195  0.239801   \n",
       "ADV   0.068988  0.346932  0.022886  0.014594  0.006965  0.014262  0.136982   \n",
       "NUM   0.003264  0.017804  0.211573  0.001187  0.013353  0.027893  0.116024   \n",
       "ADP   0.324200  0.007996  0.035394  0.069510  0.000959  0.001493  0.039659   \n",
       "\n",
       "           ADJ      NOUN       ADV       NUM       ADP  \n",
       "DET   0.204616  0.638134  0.012743  0.022121  0.009377  \n",
       "VERB  0.066052  0.109853  0.081014  0.022405  0.091247  \n",
       "X     0.016661  0.061726  0.026341  0.002698  0.144240  \n",
       "PRON  0.074952  0.208031  0.034034  0.007266  0.023327  \n",
       "CONJ  0.117867  0.344715  0.056127  0.041628  0.053789  \n",
       "PRT   0.084829  0.246982  0.010114  0.057096  0.020881  \n",
       ".     0.044226  0.220141  0.053594  0.080886  0.091425  \n",
       "ADJ   0.066798  0.701218  0.004607  0.021224  0.076505  \n",
       "NOUN  0.011807  0.263525  0.016998  0.009285  0.177438  \n",
       "ADV   0.126036  0.032504  0.079270  0.030514  0.120066  \n",
       "NUM   0.033828  0.350742  0.002374  0.187240  0.034718  \n",
       "ADP   0.106290  0.320789  0.013113  0.063220  0.017377  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DET     0.174383\n",
       "VERB    0.088453\n",
       "X       0.027112\n",
       "PRON    0.066294\n",
       "CONJ    0.057827\n",
       "PRT     0.002252\n",
       ".       0.093317\n",
       "ADJ     0.044226\n",
       "NOUN    0.220141\n",
       "ADV     0.053594\n",
       "NUM     0.080886\n",
       "ADP     0.091425\n",
       "Name: ., dtype: float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.loc['.', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "\n",
    "no_tag=[]\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "           \n",
    "        #state probabilities and hence pmax will be 0 whenever a new word is encountered        \n",
    "        if pmax == 0 :\n",
    "            no_tag.append(words[key])\n",
    "         \n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('In', 'ADP'),\n",
       "  ('major', 'ADJ'),\n",
       "  ('market', 'NOUN'),\n",
       "  ('activity', 'NOUN'),\n",
       "  (':', '.')],\n",
       " [('On', 'ADP'),\n",
       "  ('London', 'NOUN'),\n",
       "  (\"'s\", 'PRT'),\n",
       "  ('Stock', 'NOUN'),\n",
       "  ('Exchange', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('Reuters', 'NOUN'),\n",
       "  ('shares', 'NOUN'),\n",
       "  ('rose', 'VERB'),\n",
       "  ('five', 'NUM'),\n",
       "  ('pence', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('913', 'NUM'),\n",
       "  ('pence', 'NOUN'),\n",
       "  ('-LRB-', '.'),\n",
       "  ('$', '.'),\n",
       "  ('14.43', 'NUM'),\n",
       "  ('*U*', 'X'),\n",
       "  ('-RRB-', '.'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DET'),\n",
       "  ('court', 'NOUN'),\n",
       "  ('hearing', 'NOUN'),\n",
       "  ('began', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('early', 'ADJ'),\n",
       "  ('October', 'NOUN'),\n",
       "  ('at', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('request', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Anthony', 'NOUN'),\n",
       "  ('Hazell', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('district', 'NOUN'),\n",
       "  ('auditor', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('Hammersmith', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('who', 'PRON'),\n",
       "  ('*T*-63', 'X'),\n",
       "  ('argued', 'VERB'),\n",
       "  ('that', 'ADP'),\n",
       "  ('local', 'ADJ'),\n",
       "  ('councils', 'NOUN'),\n",
       "  ('are', 'VERB'),\n",
       "  (\"n't\", 'ADV'),\n",
       "  ('vested', 'VERB'),\n",
       "  ('with', 'ADP'),\n",
       "  ('constitutional', 'ADJ'),\n",
       "  ('authority', 'NOUN'),\n",
       "  ('*', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('engage', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('such', 'ADJ'),\n",
       "  ('capital-markets', 'ADJ'),\n",
       "  ('activities', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DET'),\n",
       "  ('Ministry', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('International', 'NOUN'),\n",
       "  ('Trade', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('Industry', 'NOUN'),\n",
       "  ('summoned', 'VERB'),\n",
       "  ('executives', 'NOUN'),\n",
       "  ('from', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('companies', 'NOUN'),\n",
       "  ('*-1', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('``', '.'),\n",
       "  ('make', 'VERB'),\n",
       "  ('sure', 'ADJ'),\n",
       "  ('0', 'X'),\n",
       "  ('they', 'PRON'),\n",
       "  ('understood', 'VERB'),\n",
       "  (\"''\", '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('concern', 'NOUN'),\n",
       "  ('about', 'ADP'),\n",
       "  ('such', 'ADJ'),\n",
       "  ('practices', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('according', 'VERB'),\n",
       "  ('to', 'PRT'),\n",
       "  ('a', 'DET'),\n",
       "  ('government', 'NOUN'),\n",
       "  ('spokesman', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('Cartoonist', 'NOUN'),\n",
       "  ('Garry', 'NOUN'),\n",
       "  ('Trudeau', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('suing', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('Writers', 'NOUN'),\n",
       "  ('Guild', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('America', 'NOUN'),\n",
       "  ('East', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('$', '.'),\n",
       "  ('11', 'NUM'),\n",
       "  ('million', 'NUM'),\n",
       "  ('*U*', 'X'),\n",
       "  (',', '.'),\n",
       "  ('*-1', 'X'),\n",
       "  ('alleging', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('it', 'PRON'),\n",
       "  ('mounted', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('``', '.'),\n",
       "  ('campaign', 'NOUN'),\n",
       "  ('*', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('harass', 'VERB'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('punish', 'VERB'),\n",
       "  (\"''\", '.'),\n",
       "  ('him', 'PRON'),\n",
       "  ('for', 'ADP'),\n",
       "  ('*', 'X'),\n",
       "  ('crossing', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('screenwriters', 'NOUN'),\n",
       "  (\"'\", 'PRT'),\n",
       "  ('picket', 'NOUN'),\n",
       "  ('line', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1000)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1,len(val_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [val_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  26.10124897956848\n",
      "[('In', 'ADP'), ('major', 'ADJ'), ('market', 'NOUN'), ('activity', 'NOUN'), (':', '.'), ('On', 'ADP'), ('London', 'NOUN'), (\"'s\", 'PRT'), ('Stock', 'NOUN'), ('Exchange', 'NOUN'), (',', '.'), ('Reuters', 'NOUN'), ('shares', 'NOUN'), ('rose', 'VERB'), ('five', 'NUM'), ('pence', 'NOUN'), ('to', 'PRT'), ('913', 'DET'), ('pence', 'NOUN'), ('-LRB-', '.'), ('$', '.'), ('14.43', 'DET'), ('*U*', 'X'), ('-RRB-', '.'), ('.', '.'), ('The', 'DET'), ('court', 'NOUN'), ('hearing', 'NOUN'), ('began', 'VERB'), ('in', 'ADP')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(tagged_seq[0:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Viterbi Algorithm's Accuracy : 0.9202898550724637\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy_vanilla_Viterbi = len(check)/len(tagged_seq)\n",
    "accuracy_vanilla_Viterbi\n",
    "\n",
    "print(\"Vanilla Viterbi Algorithm's Accuracy :\",accuracy_vanilla_Viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('to', 'PRT'), (('913', 'DET'), ('913', 'NUM'))],\n",
       " [('$', '.'), (('14.43', 'DET'), ('14.43', 'NUM'))],\n",
       " [('Anthony', 'NOUN'), (('Hazell', 'DET'), ('Hazell', 'NOUN'))],\n",
       " [('district', 'NOUN'), (('auditor', 'DET'), ('auditor', 'NOUN'))],\n",
       " [(\"n't\", 'ADV'), (('vested', 'DET'), ('vested', 'VERB'))],\n",
       " [('Industry', 'NOUN'), (('summoned', 'DET'), ('summoned', 'VERB'))],\n",
       " [('.', '.'), (('Cartoonist', 'DET'), ('Cartoonist', 'NOUN'))],\n",
       " [('Cartoonist', 'NOUN'), (('Garry', 'DET'), ('Garry', 'NOUN'))],\n",
       " [('and', 'CONJ'), (('punish', 'DET'), ('punish', 'VERB'))],\n",
       " [('a', 'DET'), (('screenwriters', 'DET'), ('screenwriters', 'NOUN'))],\n",
       " [('screenwriters', 'NOUN'), ((\"'\", '.'), (\"'\", 'PRT'))]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['913', '14.43', 'Hazell', 'auditor', 'vested', 'summoned', 'Cartoonist', 'Garry', 'punish', 'screenwriters']\n"
     ]
    }
   ],
   "source": [
    "##Entries which were not tagged\n",
    "print(no_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing Vanilla Viterbi Algorithm on Test Sentences\n",
    "sentence_test = 'Twitter is the best networking social site.Man is a social animal. Data science is an emerging field.Data science jobs are high in demand.'\n",
    "words = word_tokenize(sentence_test)\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('best', 'ADJ'), ('networking', 'NOUN'), ('social', 'ADJ'), ('site.Man', 'DET'), ('is', 'VERB'), ('a', 'DET'), ('social', 'ADJ'), ('animal', 'DET'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('emerging', 'VERB'), ('field.Data', 'DET'), ('science', 'NOUN'), ('jobs', 'NOUN'), ('are', 'VERB'), ('high', 'ADJ'), ('in', 'ADP'), ('demand', 'NOUN'), ('.', '.')]\n",
      "4.647507190704346\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'.': 11715,\n",
       "         'ADJ': 6397,\n",
       "         'ADP': 9857,\n",
       "         'ADV': 3171,\n",
       "         'CONJ': 2265,\n",
       "         'DET': 8725,\n",
       "         'NOUN': 28867,\n",
       "         'NUM': 3546,\n",
       "         'PRON': 2737,\n",
       "         'PRT': 3219,\n",
       "         'VERB': 13564,\n",
       "         'X': 6613})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#approach 1\n",
    "\n",
    "### Printing the unique tag and counter \n",
    "tagged_words = [tup for sent in nltk_data for tup in sent]\n",
    "tags = [pair[1] for pair in tagged_words]\n",
    "tagged_counts=Counter(tags)\n",
    "tagged_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8987693529178246"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###  Lexicon and Rule Based Tagging\n",
    "# Lexicon (or unigram tagger)\n",
    "unigram_tagger = nltk.UnigramTagger(train_set)\n",
    "unigram_tagger.evaluate(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3445811830091306"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify patterns for tagging\n",
    "# example from the NLTK book\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*ed$', 'VERB'),               # past tense\n",
    "    (r'.*es$', 'VERB'),               # 3rd singular present\n",
    "    (r'.*ould$', 'VERB'),              # modals\n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'^[A_Z].*','NOUN'),\n",
    "    (r'.*', 'NOUN')                    # nouns\n",
    "]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "# help(regexp_tagger)\n",
    "regexp_tagger.evaluate(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon and Rule Based Tagging Algorithm's Accuracy : 0.9491861849940453\n"
     ]
    }
   ],
   "source": [
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "# lexicon backed up by the rule-based tagger\n",
    "lexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n",
    "\n",
    "print(\"Lexicon and Rule Based Tagging Algorithm's Accuracy :\",lexicon_tagger.evaluate(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_unknown_word(word):\n",
    "    \n",
    "    # fetch tag based on the lexicon & rule based tagger\n",
    "    res = [val[1] for val in rule_based_tagger.tag([word])]\n",
    "    \n",
    "    # return result\n",
    "    return(str(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(tag_unknow_word('Google'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Viterbi Heuristic\n",
    "def Modified_Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # If pmax is zero that means it is a UNKNOWN WORD\n",
    "\n",
    "        if pmax == 0 :            \n",
    "                state_max = tag_unknow_word(words[key])               \n",
    "        else :\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)]  \n",
    "        state.append(state_max)\n",
    " \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  25.14567542076111\n",
      "[('In', 'ADP'), ('major', 'ADJ'), ('market', 'NOUN'), ('activity', 'NOUN'), (':', '.'), ('On', 'ADP'), ('London', 'NOUN'), (\"'s\", 'PRT'), ('Stock', 'NOUN'), ('Exchange', 'NOUN'), (',', '.'), ('Reuters', 'NOUN'), ('shares', 'NOUN'), ('rose', 'VERB'), ('five', 'NUM'), ('pence', 'NOUN'), ('to', 'PRT'), ('913', 'NUM'), ('pence', 'NOUN'), ('-LRB-', '.'), ('$', '.'), ('14.43', 'NUM'), ('*U*', 'X'), ('-RRB-', '.'), ('.', '.'), ('The', 'DET'), ('court', 'NOUN'), ('hearing', 'NOUN'), ('began', 'VERB'), ('in', 'ADP'), ('early', 'ADJ'), ('October', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('request', 'NOUN'), ('of', 'ADP'), ('Anthony', 'NOUN'), ('Hazell', 'NOUN'), (',', '.'), ('district', 'NOUN'), ('auditor', 'NOUN'), ('for', 'ADP'), ('Hammersmith', 'NOUN'), (',', '.'), ('who', 'PRON'), ('*T*-63', 'X'), ('argued', 'VERB'), ('that', 'ADP'), ('local', 'ADJ'), ('councils', 'NOUN'), ('are', 'VERB'), (\"n't\", 'ADV'), ('vested', 'VERB'), ('with', 'ADP'), ('constitutional', 'ADJ'), ('authority', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('engage', 'VERB'), ('in', 'ADP'), ('such', 'ADJ'), ('capital-markets', 'ADJ'), ('activities', 'NOUN'), ('.', '.'), ('The', 'DET'), ('Ministry', 'NOUN'), ('of', 'ADP'), ('International', 'NOUN'), ('Trade', 'NOUN'), ('and', 'CONJ'), ('Industry', 'NOUN'), ('summoned', 'VERB'), ('executives', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('companies', 'NOUN'), ('*-1', 'X'), ('to', 'PRT'), ('``', '.'), ('make', 'VERB'), ('sure', 'ADJ'), ('0', 'X'), ('they', 'PRON'), ('understood', 'VERB'), (\"''\", '.'), ('the', 'DET'), ('concern', 'NOUN'), ('about', 'ADP'), ('such', 'ADJ'), ('practices', 'NOUN'), (',', '.'), ('according', 'VERB'), ('to', 'PRT'), ('a', 'DET'), ('government', 'NOUN'), ('spokesman', 'NOUN'), ('.', '.'), ('Cartoonist', 'NOUN'), ('Garry', 'NOUN'), ('Trudeau', 'NOUN'), ('is', 'VERB'), ('suing', 'VERB'), ('the', 'DET'), ('Writers', 'NOUN'), ('Guild', 'NOUN'), ('of', 'ADP'), ('America', 'NOUN'), ('East', 'NOUN'), ('for', 'ADP'), ('$', '.'), ('11', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), (',', '.'), ('*-1', 'X'), ('alleging', 'VERB'), ('0', 'X'), ('it', 'PRON'), ('mounted', 'VERB'), ('a', 'DET'), ('``', '.'), ('campaign', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('harass', 'VERB'), ('and', 'CONJ'), ('punish', 'NOUN'), (\"''\", '.'), ('him', 'PRON'), ('for', 'ADP'), ('*', 'X'), ('crossing', 'VERB'), ('a', 'DET'), ('screenwriters', 'NOUN'), (\"'\", 'PRT'), ('picket', 'NOUN'), ('line', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# tagging the test sentences with modified_viterbi\n",
    "start = time.time()\n",
    "test_tagged_seq_using_modified_Viterbi = Modified_Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "print(test_tagged_seq_using_modified_Viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Viterbi Algorithm's Accuracy : 0.9927536231884058\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(test_tagged_seq_using_modified_Viterbi, test_run_base) if i == j] \n",
    "accuracy_modified_Viterbi = len(check)/len(test_tagged_seq_using_modified_Viterbi)\n",
    "accuracy_modified_Viterbi\n",
    "\n",
    "print(\"Modified Viterbi Algorithm's Accuracy :\",accuracy_modified_Viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('and', 'CONJ'), (('punish', 'NOUN'), ('punish', 'VERB'))]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(test_tagged_seq_using_modified_Viterbi, test_run_base)) if j[0]!=j[1]]\n",
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('best', 'ADJ'), ('networking', 'NOUN'), ('social', 'ADJ'), ('site.Man', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('social', 'ADJ'), ('animal', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('emerging', 'VERB'), ('field.Data', 'NOUN'), ('science', 'NOUN'), ('jobs', 'NOUN'), ('are', 'VERB'), ('high', 'ADJ'), ('in', 'ADP'), ('demand', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Testing on Vanilla Viterbi Algorithm on Test Sentences\n",
    "sentence_test = 'Twitter is the best networking social site.Man is a social animal. Data science is an emerging field.Data science jobs are high in demand.'\n",
    "# Android is a mobile operating system developed by Google.\n",
    "words = word_tokenize(sentence_test)\n",
    "start = time.time()\n",
    "tagged_seq = Modified_Viterbi(words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Viterbi Algorithm's Accuracy : 0.9202898550724637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Vanilla Viterbi Algorithm's Accuracy :\",accuracy_vanilla_Viterbi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Viterbi Algorithm's Accuracy : 0.9927536231884058\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Modified Viterbi Algorithm's Accuracy :\",accuracy_modified_Viterbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Original-'Twitter', '.' Corrected - 'Twitter', 'NOUN'\n",
    "2.Original Pos Tagger-'google', '.' Corrected- 'site', 'NOUN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
