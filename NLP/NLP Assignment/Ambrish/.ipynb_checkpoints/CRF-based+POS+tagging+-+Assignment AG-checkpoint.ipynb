{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech tagging using CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Ambrish\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Ambrish\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#Code :Part-of-Speech tagging using CRF\n",
    "#Author : Ambrish Gupta\n",
    "#Date : Aug 01, 2018\n",
    "###################\n",
    "\n",
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "import requests\n",
    "import seaborn as sns\n",
    "nltk.download('universal_tagset')\n",
    "from collections import Counter\n",
    "nltk.download('conll2000')\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n",
      "3914\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(wsj[:2])\n",
    "print(len(wsj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the Brown tagged sentences\n",
    "brown= list(nltk.corpus.brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')]]\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(brown[:2])\n",
    "print(len(brown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the conll2000 tagged sentences\n",
    "conll2000= list(nltk.corpus.conll2000.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Confidence', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('pound', 'NOUN'), ('is', 'VERB'), ('widely', 'ADV'), ('expected', 'VERB'), ('to', 'PRT'), ('take', 'VERB'), ('another', 'DET'), ('sharp', 'ADJ'), ('dive', 'NOUN'), ('if', 'ADP'), ('trade', 'NOUN'), ('figures', 'NOUN'), ('for', 'ADP'), ('September', 'NOUN'), (',', '.'), ('due', 'ADJ'), ('for', 'ADP'), ('release', 'NOUN'), ('tomorrow', 'NOUN'), (',', '.'), ('fail', 'VERB'), ('to', 'PRT'), ('show', 'VERB'), ('a', 'DET'), ('substantial', 'ADJ'), ('improvement', 'NOUN'), ('from', 'ADP'), ('July', 'NOUN'), ('and', 'CONJ'), ('August', 'NOUN'), (\"'s\", 'PRT'), ('near-record', 'ADJ'), ('deficits', 'NOUN'), ('.', '.')], [('Chancellor', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('Exchequer', 'NOUN'), ('Nigel', 'NOUN'), ('Lawson', 'NOUN'), (\"'s\", 'PRT'), ('restated', 'VERB'), ('commitment', 'NOUN'), ('to', 'PRT'), ('a', 'DET'), ('firm', 'NOUN'), ('monetary', 'ADJ'), ('policy', 'NOUN'), ('has', 'VERB'), ('helped', 'VERB'), ('to', 'PRT'), ('prevent', 'VERB'), ('a', 'DET'), ('freefall', 'NOUN'), ('in', 'ADP'), ('sterling', 'NOUN'), ('over', 'ADP'), ('the', 'DET'), ('past', 'ADJ'), ('week', 'NOUN'), ('.', '.')]]\n",
      "10948\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(conll2000[:2])\n",
    "print(len(conll2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_data = wsj + brown + conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]\n",
      "72202\n"
     ]
    }
   ],
   "source": [
    "print(nltk_data[1])\n",
    "print(len(nltk_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "     \n",
    "\n",
    "    features = {\n",
    "        'word':word,\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],        \n",
    "    }\n",
    "         \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "             'word':word,\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            'word':word,\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [postag for token, postag in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57761\n",
      "14441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Still', 'ADV'),\n",
       " (',', '.'),\n",
       " ('says', 'VERB'),\n",
       " ('Mr.', 'NOUN'),\n",
       " ('Lee', 'NOUN'),\n",
       " (':', '.'),\n",
       " ('``', '.'),\n",
       " ('We', 'PRON'),\n",
       " ('need', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('educate', 'VERB'),\n",
       " ('people', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('they', 'PRON'),\n",
       " ('need', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('get', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('a', 'DET'),\n",
       " ('phone', 'NOUN'),\n",
       " ('somehow', 'ADV'),\n",
       " (',', '.'),\n",
       " ('some', 'DET'),\n",
       " ('way', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('to', 'PRT'),\n",
       " ('let', 'VERB'),\n",
       " ('someone', 'NOUN'),\n",
       " ('know', 'NOUN'),\n",
       " ('what', 'PRON'),\n",
       " ('their', 'PRON'),\n",
       " ('status', 'NOUN'),\n",
       " ('is', 'NOUN'),\n",
       " ('.', '.'),\n",
       " (\"''\", '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "random.seed(1000)\n",
    "train_set, val_set = train_test_split(nltk_data,test_size=0.2)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "train_set[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('``', '.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'+1:postag': 'VERB',\n",
       " '+1:postag[:2]': 'VE',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:word.lower()': 'would',\n",
       " '-1:postag': '.',\n",
       " '-1:postag[:2]': '.',\n",
       " '-1:word.istitle()': False,\n",
       " '-1:word.isupper()': False,\n",
       " '-1:word.lower()': '``',\n",
       " 'bias': 1.0,\n",
       " 'postag': 'NOUN',\n",
       " 'postag[:2]': 'NO',\n",
       " 'word': 'Buster',\n",
       " 'word.isdigit()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isupper()': False,\n",
       " 'word.lower()': 'buster',\n",
       " 'word[-2:]': 'er',\n",
       " 'word[-3:]': 'ter'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_set[1][6])\n",
    "word2features(train_set[6],i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creating Features from sentence\n",
    "x_train= [sent2features(s) for s in train_set]\n",
    "y_train= [sent2labels(s) for s in train_set]\n",
    "\n",
    "x_val= [sent2features(s) for s in val_set]\n",
    "y_val= [sent2labels(s) for s in val_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build your CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.01, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=5,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf= sklearn_crfsuite.CRF(\n",
    "     algorithm= 'lbfgs',\n",
    "     c1=.01,\n",
    "     c2=.1,\n",
    "     max_iterations=5,\n",
    "     all_possible_transitions=True)\n",
    "\n",
    "crf.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " '.',\n",
       " 'X',\n",
       " 'ADV',\n",
       " 'PRT',\n",
       " 'CONJ',\n",
       " 'ADP',\n",
       " 'PRON',\n",
       " 'NUM']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=list(crf.classes_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9952961627962746"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=crf.predict(x_val)\n",
    "metrics.flat_f1_score(y_val,y_pred,average='weighted',labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .      1.000     1.000     1.000     38354\n",
      "          X      1.000     0.347     0.516      1638\n",
      "        ADJ      0.992     1.000     0.996     21336\n",
      "        ADP      0.996     1.000     0.998     36407\n",
      "        ADV      0.990     0.993     0.992     13774\n",
      "       VERB      0.998     1.000     0.999     46549\n",
      "        DET      1.000     1.000     1.000     33939\n",
      "       CONJ      0.998     1.000     0.999      9435\n",
      "       NOUN      0.996     1.000     0.998     76657\n",
      "       PRON      0.978     1.000     0.989     11976\n",
      "        PRT      0.999     0.999     0.999      8378\n",
      "        NUM      0.995     1.000     0.997      5639\n",
      "\n",
      "avg / total      0.996     0.996     0.995    304082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_labels=sorted(labels,key=lambda name: (name[1:],name[0]))\n",
    "print(metrics.flat_classification_report(y_val,y_pred,labels=sorted_labels,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "crf= sklearn_crfsuite.CRF(\n",
    "     algorithm= 'lbfgs',\n",
    "     max_iterations=5,\n",
    "     all_possible_transitions=True)\n",
    "\n",
    "params_space= {\n",
    "      'c1': [0.01,0.05,0.1],\n",
    "      'c2': [0.01,0.05,0.1]\n",
    "}\n",
    "\n",
    "f1_scorer=scorers.make_scorer(metrics.flat_f1_score,average='weighted',labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  27 out of  27 | elapsed: 20.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, error_score='raise',\n",
      "       estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
      "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=None,\n",
      "  calibration_candidates=None, calibration_eta=None,\n",
      "  calibration_max_trials=None, calibration_rate=None,\n",
      "  calibration_samples=None, delta=None, epsilon=None, error...e,\n",
      "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
      "  variance=None, verbose=False),\n",
      "       fit_params=None, iid=True, n_jobs=2,\n",
      "       param_grid={'c1': [0.01, 0.05, 0.1], 'c2': [0.01, 0.05, 0.1]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=make_scorer(flat_f1_score, average=weighted, labels=['DET', 'NOUN', 'VERB', 'ADJ', '.', 'X', 'ADV', 'PRT', 'CONJ', 'ADP', 'PRON', 'NUM']),\n",
      "       verbose=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_c1</th>\n",
       "      <th>param_c2</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.852669</td>\n",
       "      <td>13.355213</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.01}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>0.995419</td>\n",
       "      <td>0.913434</td>\n",
       "      <td>0.117434</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.291654</td>\n",
       "      <td>12.568610</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.05}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>0.995419</td>\n",
       "      <td>0.797873</td>\n",
       "      <td>0.582431</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.295640</td>\n",
       "      <td>11.112801</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.1}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>0.995419</td>\n",
       "      <td>1.281770</td>\n",
       "      <td>0.544614</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.347105</td>\n",
       "      <td>10.377670</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.05, 'c2': 0.01}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>2.494879</td>\n",
       "      <td>1.030107</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.401132</td>\n",
       "      <td>9.716918</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'c1': 0.05, 'c2': 0.05}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.819362</td>\n",
       "      <td>0.173702</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.500057</td>\n",
       "      <td>10.251196</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.05, 'c2': 0.1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.146832</td>\n",
       "      <td>0.350189</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.445702</td>\n",
       "      <td>9.971010</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.01}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.316402</td>\n",
       "      <td>0.020139</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42.011517</td>\n",
       "      <td>9.677885</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.05}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.622285</td>\n",
       "      <td>0.067819</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39.394069</td>\n",
       "      <td>9.153080</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995304</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.99543</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>4.534944</td>\n",
       "      <td>1.429428</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_c1  \\\n",
       "0      57.852669        13.355213         0.995414          0.995429     0.01   \n",
       "1      56.291654        12.568610         0.995414          0.995429     0.01   \n",
       "2      50.295640        11.112801         0.995414          0.995429     0.01   \n",
       "3      45.347105        10.377670         0.995422          0.995432     0.05   \n",
       "4      42.401132         9.716918         0.995422          0.995432     0.05   \n",
       "5      42.500057        10.251196         0.995422          0.995432     0.05   \n",
       "6      42.445702         9.971010         0.995422          0.995432      0.1   \n",
       "7      42.011517         9.677885         0.995422          0.995432      0.1   \n",
       "8      39.394069         9.153080         0.995422          0.995432      0.1   \n",
       "\n",
       "  param_c2                    params  rank_test_score  split0_test_score  \\\n",
       "0     0.01  {'c1': 0.01, 'c2': 0.01}                7           0.995304   \n",
       "1     0.05  {'c1': 0.01, 'c2': 0.05}                7           0.995304   \n",
       "2      0.1   {'c1': 0.01, 'c2': 0.1}                7           0.995304   \n",
       "3     0.01  {'c1': 0.05, 'c2': 0.01}                1           0.995304   \n",
       "4     0.05  {'c1': 0.05, 'c2': 0.05}                1           0.995304   \n",
       "5      0.1   {'c1': 0.05, 'c2': 0.1}                1           0.995304   \n",
       "6     0.01   {'c1': 0.1, 'c2': 0.01}                1           0.995304   \n",
       "7     0.05   {'c1': 0.1, 'c2': 0.05}                1           0.995304   \n",
       "8      0.1    {'c1': 0.1, 'c2': 0.1}                1           0.995304   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "0            0.995439           0.995134             0.99543   \n",
       "1            0.995439           0.995134             0.99543   \n",
       "2            0.995439           0.995134             0.99543   \n",
       "3            0.995439           0.995134             0.99543   \n",
       "4            0.995439           0.995134             0.99543   \n",
       "5            0.995439           0.995134             0.99543   \n",
       "6            0.995439           0.995134             0.99543   \n",
       "7            0.995439           0.995134             0.99543   \n",
       "8            0.995439           0.995134             0.99543   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.995805            0.995419      0.913434        0.117434   \n",
       "1           0.995805            0.995419      0.797873        0.582431   \n",
       "2           0.995805            0.995419      1.281770        0.544614   \n",
       "3           0.995828            0.995429      2.494879        1.030107   \n",
       "4           0.995828            0.995429      0.819362        0.173702   \n",
       "5           0.995828            0.995429      0.146832        0.350189   \n",
       "6           0.995828            0.995429      0.316402        0.020139   \n",
       "7           0.995828            0.995429      0.622285        0.067819   \n",
       "8           0.995828            0.995429      4.534944        1.429428   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.000285         0.000008  \n",
       "1        0.000285         0.000008  \n",
       "2        0.000285         0.000008  \n",
       "3        0.000295         0.000005  \n",
       "4        0.000295         0.000005  \n",
       "5        0.000295         0.000005  \n",
       "6        0.000295         0.000005  \n",
       "7        0.000295         0.000005  \n",
       "8        0.000295         0.000005  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_gs=GridSearchCV(crf,\n",
    "                   params_space,\n",
    "                   cv=3,\n",
    "                   verbose=1,\n",
    "                   n_jobs=2,\n",
    "                   scoring=f1_scorer,\n",
    "                   return_train_score=True)\n",
    "\n",
    "print(crf_gs.fit(x_train,y_train))\n",
    "\n",
    "cv_results=pd.DataFrame(crf_gs.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.05, 'c2': 0.01}\n",
      "model size: 21.82M\n"
     ]
    }
   ],
   "source": [
    "print('best params:', crf_gs.best_params_)\n",
    "print('model size: {:0.2f}M'.format(crf_gs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .      1.000     1.000     1.000     38354\n",
      "          X      1.000     0.347     0.516      1638\n",
      "        ADJ      0.992     1.000     0.996     21336\n",
      "        ADP      0.996     1.000     0.998     36407\n",
      "        ADV      0.990     0.993     0.992     13774\n",
      "       VERB      0.998     1.000     0.999     46549\n",
      "        DET      1.000     1.000     1.000     33939\n",
      "       CONJ      0.998     1.000     0.999      9435\n",
      "       NOUN      0.996     1.000     0.998     76657\n",
      "       PRON      0.978     1.000     0.989     11976\n",
      "        PRT      0.999     0.999     0.999      8378\n",
      "        NUM      0.995     1.000     0.997      5639\n",
      "\n",
      "avg / total      0.996     0.996     0.995    304082\n",
      "\n",
      "['DET', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', '.', 'DET', 'VERB', 'DET', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADV', 'ADJ', 'NOUN', 'NOUN', '.', 'ADV', 'PRON', 'VERB', 'VERB', 'VERB', 'ADP', 'ADJ', 'NOUN', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'DET',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADV',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'ADV',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_best = crf_gs.best_estimator_\n",
    "y_pred = crf_best.predict(x_val)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_val, y_pred, labels=sorted_labels, digits=3\n",
    "))\n",
    "print(y_val[1])\n",
    "y_pred[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpret the model (enlist important state and transition features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "ADJ    -> NOUN    0.612460\n",
      "PRON   -> VERB    0.605355\n",
      "DET    -> ADJ     0.460127\n",
      "NUM    -> NOUN    0.455877\n",
      "PRT    -> VERB    0.427282\n",
      "NOUN   -> CONJ    0.388977\n",
      "VERB   -> PRT     0.331743\n",
      "ADP    -> PRON    0.299310\n",
      "ADP    -> DET     0.270844\n",
      "ADV    -> VERB    0.263639\n",
      "DET    -> NOUN    0.257396\n",
      "NOUN   -> PRT     0.248911\n",
      "NOUN   -> ADP     0.220568\n",
      "VERB   -> PRON    0.186248\n",
      "VERB   -> ADV     0.181149\n",
      "ADV    -> .       0.174083\n",
      "ADP    -> ADJ     0.156679\n",
      ".      -> CONJ    0.156562\n",
      "CONJ   -> NOUN    0.148155\n",
      "ADP    -> NUM     0.146557\n",
      "\n",
      "Top unlikely transitions:\n",
      "NOUN   -> PRON    -0.109935\n",
      "ADJ    -> PRON    -0.113782\n",
      "DET    -> PRT     -0.119776\n",
      "DET    -> CONJ    -0.129186\n",
      "PRON   -> NOUN    -0.137379\n",
      "ADP    -> CONJ    -0.147783\n",
      "NOUN   -> X       -0.149131\n",
      "DET    -> .       -0.160485\n",
      "ADJ    -> DET     -0.167330\n",
      "ADJ    -> VERB    -0.171769\n",
      "X      -> NOUN    -0.183606\n",
      ".      -> NOUN    -0.188875\n",
      "ADP    -> ADP     -0.207769\n",
      "DET    -> DET     -0.210586\n",
      "NOUN   -> ADJ     -0.217678\n",
      "ADP    -> VERB    -0.229066\n",
      "ADP    -> .       -0.229156\n",
      "DET    -> ADP     -0.232474\n",
      "NOUN   -> DET     -0.235298\n",
      "ADV    -> NOUN    -0.263015\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf_best.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf_best.transition_features_).most_common()[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "3.742310 NOUN     postag:NOUN\n",
      "3.742310 NOUN     postag[:2]:NO\n",
      "3.113762 ADP      postag:ADP\n",
      "3.025177 VERB     postag:VERB\n",
      "3.025177 VERB     postag[:2]:VE\n",
      "2.937924 ADJ      postag:ADJ\n",
      "2.587936 ADV      postag:ADV\n",
      "2.452678 .        postag:.\n",
      "2.452678 .        postag[:2]:.\n",
      "2.306960 DET      postag:DET\n",
      "2.306960 DET      postag[:2]:DE\n",
      "2.146285 ADJ      postag[:2]:AD\n",
      "2.024966 PRON     postag:PRON\n",
      "1.954380 ADP      postag[:2]:AD\n",
      "1.741926 PRT      postag:PRT\n",
      "1.717605 PRON     postag[:2]:PR\n",
      "1.623815 NUM      postag:NUM\n",
      "1.623815 NUM      postag[:2]:NU\n",
      "1.616619 ADV      postag[:2]:AD\n",
      "1.528669 CONJ     postag:CONJ\n",
      "1.528669 CONJ     postag[:2]:CO\n",
      "1.520245 PRT      postag[:2]:PR\n",
      "0.968373 CONJ     word.lower():and\n",
      "0.958756 DET      word.lower():the\n",
      "0.957710 CONJ     word[-2:]:nd\n",
      "0.927130 CONJ     word:and\n",
      "0.923813 CONJ     word[-3:]:and\n",
      "0.862887 .        word:,\n",
      "0.862887 .        word.lower():,\n",
      "0.862887 .        word[-3:]:,\n",
      "\n",
      "Top negative:\n",
      "-0.201805 VERB     -1:postag[:2]:AD\n",
      "-0.202286 DET      -1:postag:DET\n",
      "-0.202286 DET      -1:postag[:2]:DE\n",
      "-0.204272 ADP      -1:postag:DET\n",
      "-0.204272 ADP      -1:postag[:2]:DE\n",
      "-0.205538 DET      +1:postag:ADP\n",
      "-0.213443 ADJ      -1:postag:NOUN\n",
      "-0.213443 ADJ      -1:postag[:2]:NO\n",
      "-0.216534 ADJ      +1:postag:VERB\n",
      "-0.216534 ADJ      +1:postag[:2]:VE\n",
      "-0.223778 .        +1:postag[:2]:AD\n",
      "-0.225446 NOUN     -1:postag:ADV\n",
      "-0.225639 ADV      +1:postag:NOUN\n",
      "-0.225639 ADV      +1:postag[:2]:NO\n",
      "-0.237926 VERB     word.istitle()\n",
      "-0.240592 NOUN     +1:postag:ADJ\n",
      "-0.243184 ADP      +1:postag:.\n",
      "-0.243184 ADP      +1:postag[:2]:.\n",
      "-0.246151 .        word.istitle()\n",
      "-0.253986 DET      -1:postag:NOUN\n",
      "-0.253986 DET      -1:postag[:2]:NO\n",
      "-0.258601 DET      bias\n",
      "-0.266568 ADP      +1:postag:VERB\n",
      "-0.266568 ADP      +1:postag[:2]:VE\n",
      "-0.267197 .        +1:postag:VERB\n",
      "-0.267197 .        +1:postag[:2]:VE\n",
      "-0.267963 .        +1:postag:NOUN\n",
      "-0.267963 .        +1:postag[:2]:NO\n",
      "-0.367376 ADJ      bias\n",
      "-0.426928 X        bias\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf_best.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf_best.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'CRF_based_POS_tagging_Assignment_AG.sav'\n",
    "pickle.dump(crf_best, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
