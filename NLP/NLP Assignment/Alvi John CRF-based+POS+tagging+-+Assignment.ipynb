{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech tagging using CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajohn021\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\ajohn021\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "import random\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "import pycrfsuite\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n",
      "3914\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(wsj[:2])\n",
    "print(len(wsj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brown tagged sentences\n",
    "brown= list(nltk.corpus.brown.tagged_sents(tagset='universal'))\n",
    "#conll2000 tagged sentences\n",
    "conll2000= list(nltk.corpus.conll2000.tagged_sents(tagset='universal'))\n",
    "nltk_data = wsj + brown + conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(nltk_data,test_size=0.4,random_state=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build your CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(data, i):\n",
    "    word = data[i][0]\n",
    "    postag = data[i][1]\n",
    "    # Common features for all words\n",
    "    features={\n",
    "        'lowercase_word':word.lower(),\n",
    "        'word[-3:]':word[-3:],\n",
    "        'word[-2:]':word[-2:],\n",
    "        'word.isupper':word.isupper(),\n",
    "        'word.istitle':word.istitle(),\n",
    "        'word.isdigit':word.isdigit(),\n",
    "    }\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a sentence\n",
    "    if i > 0:\n",
    "        word1 = data[i-1][0]\n",
    "        postag1 = data[i-1][1]\n",
    "        features.update({'previous.word.lower':word1.lower()})\n",
    "        features.update({'previous.word.istitle':word1.istitle()})\n",
    "        features.update({'previous.word.isupper':word1.isupper()})\n",
    "        features.update({'previous.word.isdigit':word1.isdigit()})\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        \n",
    "        features.update({'position':'START'})\n",
    "        \n",
    "        \n",
    "    # Features for words that are not\n",
    "    # at the end of a sentence\n",
    "    if i < len(data)-1:\n",
    "        word1 = data[i+1][0]\n",
    "        postag1 = data[i+1][1]\n",
    "        features.update({'next.word.lower':word1.lower()})\n",
    "        features.update({'next.word.istitle':word1.istitle()})\n",
    "        features.update({'next.word.isupper':word1.isupper()})\n",
    "        features.update({'next.word.isdigit':word1.isdigit()})\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.update({'position':'END'})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# A function for extracting features in documents\n",
    "def extract_features(data):\n",
    "    all_features=[]\n",
    "    for i in data:\n",
    "        each_feature=[]\n",
    "        for j in range(len(i)):\n",
    "            each_feature.append(word2features(i, j))\n",
    "        all_features.append(each_feature)\n",
    "    return all_features\n",
    "\n",
    "# A function for generating the list of labels for each document\n",
    "def get_labels(data):\n",
    "    all_pos=[]\n",
    "    for i in data:\n",
    "        each_pos=[]\n",
    "        for (word,postag) in i:\n",
    "            each_pos.append(postag)\n",
    "        all_pos.append(each_pos)\n",
    "    return all_pos        \n",
    "X_train = extract_features(train_set)\n",
    "y_train = get_labels(train_set)\n",
    "X_test = extract_features(val_set)\n",
    "y_test = get_labels(val_set)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm=None, all_possible_states=None, all_possible_transitions=None,\n",
       "  averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n",
       "  calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=None,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "model = CRF()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9648513416473967\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# parameters to tune\n",
    "params_space = {\n",
    "    'c1': [0.01, 0.1],\n",
    "    'c2': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = scorers.make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=None,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error...e,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'c1': [0.01, 0.1], 'c2': [0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(flat_f1_score, average=weighted), verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a GridSearchCV object\n",
    "rs = GridSearchCV(crf, \n",
    "                  params_space,\n",
    "                  cv=3,\n",
    "                  verbose=1,\n",
    "                  n_jobs=-1,\n",
    "                  scoring=f1_scorer, \n",
    "                  return_train_score=True)\n",
    "# fit\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store CV results in a DF\n",
    "cv_results = pd.DataFrame(rs.cv_results_)\n",
    "cv_results.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.01, c2=0.01,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=50,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a model with optimal hyperparams\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.01,\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a pickle file\n",
    "import _pickle as cPickle\n",
    "\n",
    "with open('Alvi_CRF.pkl', 'wb') as clf:\n",
    "    try:\n",
    "        cPickle.dump(crf, clf)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        clf.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "import _pickle as cPickle\n",
    "\n",
    "with open('Alvi_CRF.pkl', 'rb') as fid:\n",
    "    crf = cPickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969074253414234\n"
     ]
    }
   ],
   "source": [
    "labels =list(crf.classes_)\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b2a3ef3ec1a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m print(metrics.flat_classification_report(\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorted_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m ))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "# class-wise scores on validation data\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_val, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpret the model (enlist important state and transition features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "X      -> X       6.250391\n",
      "ADJ    -> NOUN    5.882493\n",
      "NOUN   -> NOUN    4.418739\n",
      "VERB   -> NOUN    4.364898\n",
      "DET    -> NOUN    3.828006\n",
      "ADP    -> NOUN    3.737994\n",
      "NUM    -> NOUN    3.295938\n",
      "ADV    -> ADJ     2.646997\n",
      "NOUN   -> VERB    2.376938\n",
      "ADJ    -> ADJ     2.253906\n",
      "\n",
      "Top unlikely transitions:\n",
      "ADP    -> CONJ    -1.637209\n",
      "DET    -> ADP     -1.645952\n",
      "DET    -> .       -1.653441\n",
      "NUM    -> PRON    -1.728971\n",
      "NUM    -> DET     -1.928503\n",
      "PRT    -> CONJ    -2.047916\n",
      "DET    -> PRT     -2.612784\n",
      "CONJ   -> CONJ    -3.387113\n",
      "CONJ   -> X       -3.636751\n",
      "CONJ   -> .       -4.429760\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "17.220056 NUM      lowercase_word:million\n",
      "17.015641 DET      lowercase_word:these\n",
      "16.687589 VERB     lowercase_word:were\n",
      "16.099488 ADV      lowercase_word:n't\n",
      "14.830224 NUM      lowercase_word:billion\n",
      "14.680093 CONJ     lowercase_word:and\n",
      "14.476164 ADP      lowercase_word:during\n",
      "14.165553 DET      lowercase_word:which\n",
      "13.829237 DET      lowercase_word:each\n",
      "13.768926 ADJ      lowercase_word:willing\n",
      "\n",
      "Top negative:\n",
      "-5.930749 VERB     lowercase_word:subject\n",
      "-6.216390 PRON     next.word.lower:things\n",
      "-6.246356 VERB     lowercase_word:down\n",
      "-6.565561 .        previous.word.lower:c.\n",
      "-6.795203 NOUN     lowercase_word:'s\n",
      "-6.948302 ADV      previous.word.lower:asia\n",
      "-7.063754 ADP      next.word.lower:down\n",
      "-7.353129 NOUN     previous.word.lower:employees\n",
      "-7.356359 DET      next.word.lower:ill\n",
      "-7.696527 ADP      next.word.lower:rear\n"
     ]
    }
   ],
   "source": [
    "# important features\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
