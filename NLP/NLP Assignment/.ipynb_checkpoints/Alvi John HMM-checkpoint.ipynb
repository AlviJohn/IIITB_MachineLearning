{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import nltk\n",
    "import re\n",
    "import pprint,time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import requests\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('universal_tagset')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3796\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(nltk_data,test_size=0.03)\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'funding',\n",
       " 'is',\n",
       " 'attached',\n",
       " '*-11',\n",
       " 'to',\n",
       " 'an',\n",
       " 'estimated',\n",
       " '$',\n",
       " '27.1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12211\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "V = set(tokens)\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 {'.', 'NOUN', 'ADJ', 'VERB', 'X', 'ADV', 'PRON', 'CONJ', 'DET', 'NUM', 'PRT', 'ADP'}\n"
     ]
    }
   ],
   "source": [
    "# number of tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "print(len(T),T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12211\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "print(t)\n",
    "v = len(V)\n",
    "print(v)\n",
    "w_given_t = np.zeros((t, v))\n",
    "print(w_given_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.093541</td>\n",
       "      <td>0.222193</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.088349</td>\n",
       "      <td>0.026839</td>\n",
       "      <td>0.052622</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>0.058958</td>\n",
       "      <td>0.174410</td>\n",
       "      <td>0.080341</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.090637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.240171</td>\n",
       "      <td>0.264846</td>\n",
       "      <td>0.012141</td>\n",
       "      <td>0.146902</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.042492</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.175897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.064667</td>\n",
       "      <td>0.700693</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.011934</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.016933</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.077407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.109997</td>\n",
       "      <td>0.065558</td>\n",
       "      <td>0.169401</td>\n",
       "      <td>0.218095</td>\n",
       "      <td>0.081814</td>\n",
       "      <td>0.035096</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.133698</td>\n",
       "      <td>0.023017</td>\n",
       "      <td>0.031222</td>\n",
       "      <td>0.091386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.163361</td>\n",
       "      <td>0.062002</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.205060</td>\n",
       "      <td>0.074184</td>\n",
       "      <td>0.026238</td>\n",
       "      <td>0.056224</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.054193</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.184445</td>\n",
       "      <td>0.144307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.134864</td>\n",
       "      <td>0.032018</td>\n",
       "      <td>0.128719</td>\n",
       "      <td>0.345084</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.078590</td>\n",
       "      <td>0.015201</td>\n",
       "      <td>0.007115</td>\n",
       "      <td>0.069534</td>\n",
       "      <td>0.031695</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>0.119987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.041541</td>\n",
       "      <td>0.208459</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.486027</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.012462</td>\n",
       "      <td>0.022281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.349048</td>\n",
       "      <td>0.116954</td>\n",
       "      <td>0.157298</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>0.058930</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.121487</td>\n",
       "      <td>0.042158</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>0.052131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.636407</td>\n",
       "      <td>0.205053</td>\n",
       "      <td>0.039665</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.009326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.115072</td>\n",
       "      <td>0.357681</td>\n",
       "      <td>0.032464</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.207536</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.184348</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.042615</td>\n",
       "      <td>0.246716</td>\n",
       "      <td>0.084909</td>\n",
       "      <td>0.400513</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.102211</td>\n",
       "      <td>0.057674</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.019225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.039551</td>\n",
       "      <td>0.320709</td>\n",
       "      <td>0.107113</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.034620</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.068611</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.324486</td>\n",
       "      <td>0.063366</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .      NOUN       ADJ      VERB         X       ADV      PRON  \\\n",
       ".     0.093541  0.222193  0.043911  0.088349  0.026839  0.052622  0.065734   \n",
       "NOUN  0.240171  0.264846  0.012141  0.146902  0.029031  0.017283  0.004642   \n",
       "ADJ   0.064667  0.700693  0.065796  0.011934  0.020642  0.004838  0.000645   \n",
       "VERB  0.035400  0.109997  0.065558  0.169401  0.218095  0.081814  0.035096   \n",
       "X     0.163361  0.062002  0.016711  0.205060  0.074184  0.026238  0.056224   \n",
       "ADV   0.134864  0.032018  0.128719  0.345084  0.022639  0.078590  0.015201   \n",
       "PRON  0.041541  0.208459  0.072885  0.486027  0.092145  0.033988  0.007553   \n",
       "CONJ  0.033998  0.349048  0.116954  0.157298  0.008613  0.054850  0.058930   \n",
       "DET   0.018062  0.636407  0.205053  0.039665  0.046039  0.012749  0.003660   \n",
       "NUM   0.115072  0.357681  0.032464  0.018261  0.207536  0.002899  0.001449   \n",
       "PRT   0.042615  0.246716  0.084909  0.400513  0.013457  0.009933  0.018584   \n",
       "ADP   0.039551  0.320709  0.107113  0.008288  0.034620  0.013848  0.068611   \n",
       "\n",
       "          CONJ       DET       NUM       PRT       ADP  \n",
       ".     0.058958  0.174410  0.080341  0.002376  0.090637  \n",
       "NOUN  0.042492  0.013212  0.009427  0.043956  0.175897  \n",
       "ADJ   0.016933  0.004838  0.020964  0.010643  0.077407  \n",
       "VERB  0.005318  0.133698  0.023017  0.031222  0.091386  \n",
       "X     0.010464  0.054193  0.002811  0.184445  0.144307  \n",
       "ADV   0.007115  0.069534  0.031695  0.014554  0.119987  \n",
       "PRON  0.005287  0.009819  0.007553  0.012462  0.022281  \n",
       "CONJ  0.000453  0.121487  0.042158  0.004080  0.052131  \n",
       "DET   0.000472  0.005784  0.022548  0.000236  0.009326  \n",
       "NUM   0.013913  0.003188  0.184348  0.027826  0.035362  \n",
       "PRT   0.002243  0.102211  0.057674  0.001922  0.019225  \n",
       "ADP   0.000839  0.324486  0.063366  0.001469  0.017100  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".       0.093541\n",
       "NOUN    0.222193\n",
       "ADJ     0.043911\n",
       "VERB    0.088349\n",
       "X       0.026839\n",
       "ADV     0.052622\n",
       "PRON    0.065734\n",
       "CONJ    0.058958\n",
       "DET     0.174410\n",
       "NUM     0.080341\n",
       "PRT     0.002376\n",
       "ADP     0.090637\n",
       "Name: ., dtype: float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.loc['.', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "                \n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = T[p.index(pmax)]\n",
    "        # getting state for which probability is maximum\n",
    "        state.append(state_max)\n",
    "        \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 83 list of untagged words ['The', 'company', 'is', 'operating', 'under', 'Chapter', '11', 'of', 'the', 'federal', 'Bankruptcy', 'Code', ',', '*', 'giving', 'it', 'court', 'protection', 'from', 'creditors', \"'\", 'lawsuits', 'while', 'it', 'attempts', '*-1', 'to', 'work', 'out', 'a', 'plan', '*', 'to', 'pay', 'its', 'debts', '.', 'Two', 'years', 'ago', ',', 'the', 'Rev.', 'Jeremy', 'Hummerstone', ',', 'vicar', 'of', 'Great', 'Torrington', ',', 'Devon', ',', 'got', 'so', 'fed', 'up', 'with', 'ringers', 'who', '*T*-228', 'did', \"n't\", 'attend', 'service', '0', 'he', 'sacked', 'the', 'entire', 'band', ';', 'the', 'ringers', 'promptly', 'set', 'up', 'a', 'picket', 'line', 'in', 'protest', '.']\n"
     ]
    }
   ],
   "source": [
    "# Running on entire test dataset would take more than 3-4hrs. \n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 20 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(2)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "#print(\"list of sents\",test_run)\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "#print(\"list of tagged words\",test_run_base)\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "print(\"len\",len(test_tagged_words),\"list of untagged words\",test_tagged_words)\n",
    "#test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  14.263831377029419\n",
      "83 [('The', 'DET'), ('company', 'NOUN'), ('is', 'VERB'), ('operating', 'VERB'), ('under', 'ADP'), ('Chapter', 'NOUN'), ('11', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('federal', 'ADJ'), ('Bankruptcy', 'NOUN'), ('Code', 'NOUN'), (',', '.'), ('*', 'X'), ('giving', 'VERB'), ('it', 'PRON'), ('court', 'NOUN'), ('protection', 'NOUN'), ('from', 'ADP'), ('creditors', 'NOUN'), (\"'\", 'PRT'), ('lawsuits', 'NOUN'), ('while', 'ADP'), ('it', 'PRON'), ('attempts', 'VERB'), ('*-1', 'X'), ('to', 'PRT'), ('work', 'VERB'), ('out', 'PRT'), ('a', 'DET'), ('plan', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('pay', 'VERB'), ('its', 'PRON'), ('debts', 'NOUN'), ('.', '.'), ('Two', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('the', 'DET'), ('Rev.', '.'), ('Jeremy', '.'), ('Hummerstone', 'NOUN'), (',', '.'), ('vicar', 'NOUN'), ('of', 'ADP'), ('Great', 'NOUN'), ('Torrington', 'NOUN'), (',', '.'), ('Devon', 'NOUN'), (',', '.'), ('got', 'VERB'), ('so', 'ADV'), ('fed', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('ringers', 'NOUN'), ('who', 'PRON'), ('*T*-228', '.'), ('did', 'VERB'), (\"n't\", 'ADV'), ('attend', 'VERB'), ('service', 'NOUN'), ('0', 'X'), ('he', 'PRON'), ('sacked', '.'), ('the', 'DET'), ('entire', 'ADJ'), ('band', 'NOUN'), (';', '.'), ('the', 'DET'), ('ringers', 'NOUN'), ('promptly', 'ADV'), ('set', 'VERB'), ('up', 'ADV'), ('a', 'DET'), ('picket', 'NOUN'), ('line', 'NOUN'), ('in', 'ADP'), ('protest', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(len(tagged_seq),tagged_seq)\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156626506024096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('years', 'NOUN'), (('ago', 'ADP'), ('ago', 'ADV'))],\n",
       " [('the', 'DET'), (('Rev.', '.'), ('Rev.', 'NOUN'))],\n",
       " [('Rev.', 'NOUN'), (('Jeremy', '.'), ('Jeremy', 'NOUN'))],\n",
       " [('fed', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))],\n",
       " [('who', 'PRON'), (('*T*-228', '.'), ('*T*-228', 'X'))],\n",
       " [('he', 'PRON'), (('sacked', '.'), ('sacked', 'VERB'))],\n",
       " [('set', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', '.'), ('is', 'VERB'), ('the', 'DET'), ('best', 'ADJ'), ('networking', 'NOUN'), ('social', 'ADJ'), ('site', '.'), ('.', '.'), ('Man', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('social', 'ADJ'), ('animal', '.'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('emerging', 'VERB'), ('field', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('jobs', 'NOUN'), ('are', 'VERB'), ('high', 'ADJ'), ('in', 'ADP'), ('demand', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "sentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\n",
    "words = nltk.word_tokenize(sentence_test)\n",
    "\n",
    "#start = time.time()\n",
    "tagged_seq = Viterbi(words)\n",
    "print(tagged_seq)\n",
    "# end = time.time()\n",
    "# difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', '.'), ('is', 'VERB'), ('the', 'DET'), ('best', 'ADJ'), ('networking', 'NOUN'), ('social', 'ADJ'), ('site', '.'), ('.', '.'), ('Man', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('social', 'ADJ'), ('animal', '.'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('emerging', 'VERB'), ('field', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('jobs', 'NOUN'), ('are', 'VERB'), ('high', 'ADJ'), ('in', 'ADP'), ('demand', 'NOUN'), ('.', '.')]\n",
      "14.263831377029419\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution1  for tagging Unknown Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can infer from the result that 'NOUN' has repeated maximum number of times.\n",
    "######  Since for a new unknown word the 'emission probability' is zero so hence the 'state probability' will also be zero\n",
    "######  So we consider by default that the tag for the word will be 'NOUN'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the the tag which has maximum frequency in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 28005, 'VERB': 13164, '.': 11364, 'ADP': 9532, 'DET': 8471, 'X': 6403, 'ADJ': 6201, 'NUM': 3450, 'PRT': 3121, 'ADV': 3092, 'PRON': 2648, 'CONJ': 2206})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tags=[pos_tag for i in train_set for (word,pos_tag) in i]\n",
    "print(Counter(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Viterbi Heuristic\n",
    "def Viterbi_solution1(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        if (pmax==0):##Check if it is an unknown word\n",
    "            state_max='NOUN'\n",
    "        else:\n",
    "            state_max = T[p.index(pmax)]\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 83 list of untagged words ['The', 'company', 'is', 'operating', 'under', 'Chapter', '11', 'of', 'the', 'federal', 'Bankruptcy', 'Code', ',', '*', 'giving', 'it', 'court', 'protection', 'from', 'creditors', \"'\", 'lawsuits', 'while', 'it', 'attempts', '*-1', 'to', 'work', 'out', 'a', 'plan', '*', 'to', 'pay', 'its', 'debts', '.', 'Two', 'years', 'ago', ',', 'the', 'Rev.', 'Jeremy', 'Hummerstone', ',', 'vicar', 'of', 'Great', 'Torrington', ',', 'Devon', ',', 'got', 'so', 'fed', 'up', 'with', 'ringers', 'who', '*T*-228', 'did', \"n't\", 'attend', 'service', '0', 'he', 'sacked', 'the', 'entire', 'band', ';', 'the', 'ringers', 'promptly', 'set', 'up', 'a', 'picket', 'line', 'in', 'protest', '.']\n"
     ]
    }
   ],
   "source": [
    "# Running on entire test dataset would take more than 3-4hrs. \n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 20 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(2)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "#print(\"list of sents\",test_run)\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "#print(\"list of tagged words\",test_run_base)\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "print(\"len\",len(test_tagged_words),\"list of untagged words\",test_tagged_words)\n",
    "#test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_solution1(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  14.187083959579468\n",
      "83 [('The', 'DET'), ('company', 'NOUN'), ('is', 'VERB'), ('operating', 'VERB'), ('under', 'ADP'), ('Chapter', 'NOUN'), ('11', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('federal', 'ADJ'), ('Bankruptcy', 'NOUN'), ('Code', 'NOUN'), (',', '.'), ('*', 'X'), ('giving', 'VERB'), ('it', 'PRON'), ('court', 'NOUN'), ('protection', 'NOUN'), ('from', 'ADP'), ('creditors', 'NOUN'), (\"'\", 'PRT'), ('lawsuits', 'NOUN'), ('while', 'ADP'), ('it', 'PRON'), ('attempts', 'VERB'), ('*-1', 'X'), ('to', 'PRT'), ('work', 'VERB'), ('out', 'PRT'), ('a', 'DET'), ('plan', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('pay', 'VERB'), ('its', 'PRON'), ('debts', 'NOUN'), ('.', '.'), ('Two', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('the', 'DET'), ('Rev.', 'NOUN'), ('Jeremy', 'NOUN'), ('Hummerstone', 'NOUN'), (',', '.'), ('vicar', 'NOUN'), ('of', 'ADP'), ('Great', 'NOUN'), ('Torrington', 'NOUN'), (',', '.'), ('Devon', 'NOUN'), (',', '.'), ('got', 'VERB'), ('so', 'ADV'), ('fed', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('ringers', 'NOUN'), ('who', 'PRON'), ('*T*-228', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('attend', 'VERB'), ('service', 'NOUN'), ('0', 'X'), ('he', 'PRON'), ('sacked', 'NOUN'), ('the', 'DET'), ('entire', 'ADJ'), ('band', 'NOUN'), (';', '.'), ('the', 'DET'), ('ringers', 'NOUN'), ('promptly', 'ADV'), ('set', 'VERB'), ('up', 'ADV'), ('a', 'DET'), ('picket', 'NOUN'), ('line', 'NOUN'), ('in', 'ADP'), ('protest', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(len(tagged_seq),tagged_seq)\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397590361445783"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('years', 'NOUN'), (('ago', 'ADP'), ('ago', 'ADV'))],\n",
       " [('fed', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))],\n",
       " [('who', 'PRON'), (('*T*-228', 'NOUN'), ('*T*-228', 'X'))],\n",
       " [('he', 'PRON'), (('sacked', 'NOUN'), ('sacked', 'VERB'))],\n",
       " [('set', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('best', 'ADJ'), ('networking', 'NOUN'), ('social', 'ADJ'), ('site', 'NOUN'), ('.', '.'), ('Man', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('social', 'ADJ'), ('animal', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('emerging', 'VERB'), ('field', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('jobs', 'NOUN'), ('are', 'VERB'), ('high', 'ADJ'), ('in', 'ADP'), ('demand', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "sentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\n",
    "words = nltk.word_tokenize(sentence_test)\n",
    "#start = time.time()\n",
    "tagged_seq = Viterbi_solution1(words)\n",
    "print(tagged_seq)\n",
    "# end = time.time()\n",
    "# difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution2  for tagging Unknown Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For any unknown word which is not in the corpus the emission probability will be zero\n",
    "###### So we ignore the emission probability and conisder only the transmission probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi_solution2(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        t = []\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "            t.append(transition_p)\n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        if pmax==0:\n",
    "            pmax=max(t)\n",
    "            state_max = T[t.index(pmax)]\n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 83 list of untagged words ['The', 'company', 'is', 'operating', 'under', 'Chapter', '11', 'of', 'the', 'federal', 'Bankruptcy', 'Code', ',', '*', 'giving', 'it', 'court', 'protection', 'from', 'creditors', \"'\", 'lawsuits', 'while', 'it', 'attempts', '*-1', 'to', 'work', 'out', 'a', 'plan', '*', 'to', 'pay', 'its', 'debts', '.', 'Two', 'years', 'ago', ',', 'the', 'Rev.', 'Jeremy', 'Hummerstone', ',', 'vicar', 'of', 'Great', 'Torrington', ',', 'Devon', ',', 'got', 'so', 'fed', 'up', 'with', 'ringers', 'who', '*T*-228', 'did', \"n't\", 'attend', 'service', '0', 'he', 'sacked', 'the', 'entire', 'band', ';', 'the', 'ringers', 'promptly', 'set', 'up', 'a', 'picket', 'line', 'in', 'protest', '.']\n"
     ]
    }
   ],
   "source": [
    "# Running on entire test dataset would take more than 3-4hrs. \n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 20 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(2)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "#print(\"list of sents\",test_run)\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "#print(\"list of tagged words\",test_run_base)\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "print(\"len\",len(test_tagged_words),\"list of untagged words\",test_tagged_words)\n",
    "#test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_solution2(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  14.121038675308228\n",
      "83 [('The', 'DET'), ('company', 'NOUN'), ('is', 'VERB'), ('operating', 'VERB'), ('under', 'ADP'), ('Chapter', 'NOUN'), ('11', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('federal', 'ADJ'), ('Bankruptcy', 'NOUN'), ('Code', 'NOUN'), (',', '.'), ('*', 'X'), ('giving', 'VERB'), ('it', 'PRON'), ('court', 'NOUN'), ('protection', 'NOUN'), ('from', 'ADP'), ('creditors', 'NOUN'), (\"'\", 'PRT'), ('lawsuits', 'NOUN'), ('while', 'ADP'), ('it', 'PRON'), ('attempts', 'VERB'), ('*-1', 'X'), ('to', 'PRT'), ('work', 'VERB'), ('out', 'PRT'), ('a', 'DET'), ('plan', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('pay', 'VERB'), ('its', 'PRON'), ('debts', 'NOUN'), ('.', '.'), ('Two', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('the', 'DET'), ('Rev.', 'NOUN'), ('Jeremy', 'NOUN'), ('Hummerstone', 'NOUN'), (',', '.'), ('vicar', 'NOUN'), ('of', 'ADP'), ('Great', 'NOUN'), ('Torrington', 'NOUN'), (',', '.'), ('Devon', 'NOUN'), (',', '.'), ('got', 'VERB'), ('so', 'ADV'), ('fed', 'VERB'), ('up', 'ADV'), ('with', 'ADP'), ('ringers', 'NOUN'), ('who', 'PRON'), ('*T*-228', 'VERB'), ('did', 'VERB'), (\"n't\", 'ADV'), ('attend', 'VERB'), ('service', 'NOUN'), ('0', 'X'), ('he', 'PRON'), ('sacked', 'VERB'), ('the', 'DET'), ('entire', 'ADJ'), ('band', 'NOUN'), (';', '.'), ('the', 'DET'), ('ringers', 'NOUN'), ('promptly', 'ADV'), ('set', 'VERB'), ('up', 'ADV'), ('a', 'DET'), ('picket', 'NOUN'), ('line', 'NOUN'), ('in', 'ADP'), ('protest', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(len(tagged_seq),tagged_seq)\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(check)/len(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9518072289156626"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('years', 'NOUN'), (('ago', 'ADP'), ('ago', 'ADV'))],\n",
       " [('fed', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))],\n",
       " [('who', 'PRON'), (('*T*-228', 'VERB'), ('*T*-228', 'X'))],\n",
       " [('set', 'VERB'), (('up', 'ADV'), ('up', 'PRT'))]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('best', 'ADJ'), ('networking', 'NOUN'), ('social', 'ADJ'), ('site', 'NOUN'), ('.', '.'), ('Man', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('social', 'ADJ'), ('animal', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('emerging', 'VERB'), ('field', 'NOUN'), ('.', '.'), ('Data', 'NOUN'), ('science', 'NOUN'), ('jobs', 'NOUN'), ('are', 'VERB'), ('high', 'ADJ'), ('in', 'ADP'), ('demand', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "sentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\n",
    "words = nltk.word_tokenize(sentence_test)\n",
    "#start = time.time()\n",
    "tagged_seq = Viterbi_solution2(words)\n",
    "print(tagged_seq)\n",
    "# end = time.time()\n",
    "# difference = end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1 Accuracy : 0.9397590361445783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2 Accuracy : 0.9518072289156626"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case 1\n",
    "Original Pos Tagger\n",
    "('Twitter', '.')\n",
    "Corrected Pos Tagger\n",
    "('Twitter', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case2\n",
    "Original Pos Tagger \n",
    "('site', '.')\n",
    "Corrected Pos Tagger\n",
    "('site', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case3\n",
    "Original Pos Tagger\n",
    "('animal', '.')\n",
    "Corrected Pos Tagger\n",
    "('animal', 'NOUN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
