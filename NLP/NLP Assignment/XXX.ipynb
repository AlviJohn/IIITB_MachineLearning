{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech tagging using CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\vibhav\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\vibhav\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "import random\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "import pycrfsuite\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n",
      "3914\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(wsj[:2])\n",
    "print(len(wsj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Brown tagged sentences\n",
    "brown= list(nltk.corpus.brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')]]\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(brown[:2])\n",
    "print(len(brown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the conll2000 tagged sentences\n",
    "conll2000= list(nltk.corpus.conll2000.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Confidence', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('pound', 'NOUN'), ('is', 'VERB'), ('widely', 'ADV'), ('expected', 'VERB'), ('to', 'PRT'), ('take', 'VERB'), ('another', 'DET'), ('sharp', 'ADJ'), ('dive', 'NOUN'), ('if', 'ADP'), ('trade', 'NOUN'), ('figures', 'NOUN'), ('for', 'ADP'), ('September', 'NOUN'), (',', '.'), ('due', 'ADJ'), ('for', 'ADP'), ('release', 'NOUN'), ('tomorrow', 'NOUN'), (',', '.'), ('fail', 'VERB'), ('to', 'PRT'), ('show', 'VERB'), ('a', 'DET'), ('substantial', 'ADJ'), ('improvement', 'NOUN'), ('from', 'ADP'), ('July', 'NOUN'), ('and', 'CONJ'), ('August', 'NOUN'), (\"'s\", 'PRT'), ('near-record', 'ADJ'), ('deficits', 'NOUN'), ('.', '.')], [('Chancellor', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('Exchequer', 'NOUN'), ('Nigel', 'NOUN'), ('Lawson', 'NOUN'), (\"'s\", 'PRT'), ('restated', 'VERB'), ('commitment', 'NOUN'), ('to', 'PRT'), ('a', 'DET'), ('firm', 'NOUN'), ('monetary', 'ADJ'), ('policy', 'NOUN'), ('has', 'VERB'), ('helped', 'VERB'), ('to', 'PRT'), ('prevent', 'VERB'), ('a', 'DET'), ('freefall', 'NOUN'), ('in', 'ADP'), ('sterling', 'NOUN'), ('over', 'ADP'), ('the', 'DET'), ('past', 'ADJ'), ('week', 'NOUN'), ('.', '.')]]\n",
      "10948\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(conll2000[:2])\n",
    "print(len(conll2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data = wsj + brown + conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43321\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set = train_test_split(nltk_data,test_size=0.4,random_state=100)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build your CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(data, i):\n",
    "    word = data[i][0]\n",
    "    postag = data[i][1]\n",
    "    # Common features for all words\n",
    "    features={\n",
    "        'lowercase_word':word.lower(),\n",
    "        'word[-3:]':word[-3:],\n",
    "        'word[-2:]':word[-2:],\n",
    "        'word.isupper':word.isupper(),\n",
    "        'word.istitle':word.istitle(),\n",
    "        'word.isdigit':word.isdigit(),\n",
    "    }\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a sentence\n",
    "    if i > 0:\n",
    "        word1 = data[i-1][0]\n",
    "        postag1 = data[i-1][1]\n",
    "        features.update({'previous.word.lower':word1.lower()})\n",
    "        features.update({'previous.word.istitle':word1.istitle()})\n",
    "        features.update({'previous.word.isupper':word1.isupper()})\n",
    "        features.update({'previous.word.isdigit':word1.isdigit()})\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        \n",
    "        features.update({'position':'START'})\n",
    "        \n",
    "        \n",
    "    # Features for words that are not\n",
    "    # at the end of a sentence\n",
    "    if i < len(data)-1:\n",
    "        word1 = data[i+1][0]\n",
    "        postag1 = data[i+1][1]\n",
    "        features.update({'next.word.lower':word1.lower()})\n",
    "        features.update({'next.word.istitle':word1.istitle()})\n",
    "        features.update({'next.word.isupper':word1.isupper()})\n",
    "        features.update({'next.word.isdigit':word1.isdigit()})\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.update({'position':'END'})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# A function for extracting features in documents\n",
    "def extract_features(data):\n",
    "    all_features=[]\n",
    "    for i in data:\n",
    "        each_feature=[]\n",
    "        for j in range(len(i)):\n",
    "            each_feature.append(word2features(i, j))\n",
    "        all_features.append(each_feature)\n",
    "    return all_features\n",
    "\n",
    "# A function for generating the list of labels for each document\n",
    "def get_labels(data):\n",
    "    all_pos=[]\n",
    "    for i in data:\n",
    "        each_pos=[]\n",
    "        for (word,postag) in i:\n",
    "            each_pos.append(postag)\n",
    "        all_pos.append(each_pos)\n",
    "    return all_pos        \n",
    "X_train = extract_features(train_set)\n",
    "y_train = get_labels(train_set)\n",
    "X_test = extract_features(val_set)\n",
    "y_test = get_labels(val_set)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm=None, all_possible_states=None, all_possible_transitions=None,\n",
       "  averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n",
       "  calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=None,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "model = CRF()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9648513416473967\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# parameters to tune\n",
    "params_space = {\n",
    "    'c1': [0.01, 0.1],\n",
    "    'c2': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = scorers.make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 13.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=None,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error...e,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'c1': [0.01, 0.1], 'c2': [0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(flat_f1_score, average=weighted), verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a GridSearchCV object\n",
    "rs = GridSearchCV(crf, \n",
    "                  params_space,\n",
    "                  cv=3,\n",
    "                  verbose=1,\n",
    "                  n_jobs=-1,\n",
    "                  scoring=f1_scorer, \n",
    "                  return_train_score=True)\n",
    "# fit\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_c1</th>\n",
       "      <th>param_c2</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213.047466</td>\n",
       "      <td>17.599016</td>\n",
       "      <td>16.345589</td>\n",
       "      <td>2.209197</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.01}</td>\n",
       "      <td>0.966229</td>\n",
       "      <td>0.967119</td>\n",
       "      <td>0.967332</td>\n",
       "      <td>0.966893</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989999</td>\n",
       "      <td>0.989785</td>\n",
       "      <td>0.990228</td>\n",
       "      <td>0.990004</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207.419033</td>\n",
       "      <td>3.089120</td>\n",
       "      <td>15.155391</td>\n",
       "      <td>0.802297</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.1}</td>\n",
       "      <td>0.966433</td>\n",
       "      <td>0.967088</td>\n",
       "      <td>0.967639</td>\n",
       "      <td>0.967053</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>2</td>\n",
       "      <td>0.987212</td>\n",
       "      <td>0.986748</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>0.987073</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191.043543</td>\n",
       "      <td>1.624542</td>\n",
       "      <td>13.425253</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.01}</td>\n",
       "      <td>0.966723</td>\n",
       "      <td>0.967036</td>\n",
       "      <td>0.967464</td>\n",
       "      <td>0.967074</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989342</td>\n",
       "      <td>0.989310</td>\n",
       "      <td>0.989180</td>\n",
       "      <td>0.989277</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183.629638</td>\n",
       "      <td>2.607905</td>\n",
       "      <td>10.086579</td>\n",
       "      <td>0.517660</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.1}</td>\n",
       "      <td>0.966261</td>\n",
       "      <td>0.966866</td>\n",
       "      <td>0.967368</td>\n",
       "      <td>0.966832</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>4</td>\n",
       "      <td>0.985903</td>\n",
       "      <td>0.985514</td>\n",
       "      <td>0.985766</td>\n",
       "      <td>0.985728</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_c1  \\\n",
       "0     213.047466     17.599016        16.345589        2.209197     0.01   \n",
       "1     207.419033      3.089120        15.155391        0.802297     0.01   \n",
       "2     191.043543      1.624542        13.425253        0.694842      0.1   \n",
       "3     183.629638      2.607905        10.086579        0.517660      0.1   \n",
       "\n",
       "  param_c2                    params  split0_test_score  split1_test_score  \\\n",
       "0     0.01  {'c1': 0.01, 'c2': 0.01}           0.966229           0.967119   \n",
       "1      0.1   {'c1': 0.01, 'c2': 0.1}           0.966433           0.967088   \n",
       "2     0.01   {'c1': 0.1, 'c2': 0.01}           0.966723           0.967036   \n",
       "3      0.1    {'c1': 0.1, 'c2': 0.1}           0.966261           0.966866   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.967332         0.966893        0.000478                3   \n",
       "1           0.967639         0.967053        0.000493                2   \n",
       "2           0.967464         0.967074        0.000303                1   \n",
       "3           0.967368         0.966832        0.000453                4   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0            0.989999            0.989785            0.990228   \n",
       "1            0.987212            0.986748            0.987258   \n",
       "2            0.989342            0.989310            0.989180   \n",
       "3            0.985903            0.985514            0.985766   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.990004         0.000181  \n",
       "1          0.987073         0.000230  \n",
       "2          0.989277         0.000070  \n",
       "3          0.985728         0.000161  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store CV results in a DF\n",
    "cv_results = pd.DataFrame(rs.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.01, c2=0.01,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=50,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a model with optimal hyperparams\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.01,\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a pickle file\n",
    "import _pickle as cPickle\n",
    "# import pickle\n",
    "# model_name='final_crf'\n",
    "# pickle.dump(model, open('C:\\Users\\vibhav\\Desktop\\Crf_Model', 'wb'))\n",
    "\n",
    "with open('Vibhav_crf.pkl', 'wb') as clf:\n",
    "    try:\n",
    "        cPickle.dump(crf, clf)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        clf.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "import _pickle as cPickle\n",
    "\n",
    "with open('Vibhav_crf.pkl', 'rb') as fid:\n",
    "    crf = cPickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969074253414234\n"
     ]
    }
   ],
   "source": [
    "labels =list(crf.classes_)\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise scores on validation data\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_val, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpret the model (enlist important state and transition features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "X      -> X       6.250391\n",
      "ADJ    -> NOUN    5.882493\n",
      "NOUN   -> NOUN    4.418739\n",
      "VERB   -> NOUN    4.364898\n",
      "DET    -> NOUN    3.828006\n",
      "ADP    -> NOUN    3.737994\n",
      "NUM    -> NOUN    3.295938\n",
      "ADV    -> ADJ     2.646997\n",
      "NOUN   -> VERB    2.376938\n",
      "ADJ    -> ADJ     2.253906\n",
      "VERB   -> ADV     2.224495\n",
      "VERB   -> ADJ     2.148090\n",
      "VERB   -> PRT     2.116329\n",
      "ADV    -> ADV     1.885738\n",
      "NOUN   -> PRON    1.884802\n",
      "ADV    -> VERB    1.884006\n",
      "PRT    -> VERB    1.817723\n",
      "PRON   -> NOUN    1.757976\n",
      "X      -> NOUN    1.722016\n",
      "PRT    -> NOUN    1.607191\n",
      "\n",
      "Top unlikely transitions:\n",
      "PRT    -> PRT     -1.386880\n",
      "PRON   -> CONJ    -1.391091\n",
      ".      -> PRON    -1.447736\n",
      "PRON   -> PRT     -1.455765\n",
      "NUM    -> VERB    -1.494329\n",
      "X      -> NUM     -1.495900\n",
      ".      -> X       -1.509421\n",
      "DET    -> PRON    -1.520167\n",
      "DET    -> CONJ    -1.549758\n",
      "DET    -> DET     -1.633011\n",
      "ADP    -> CONJ    -1.637209\n",
      "DET    -> ADP     -1.645952\n",
      "DET    -> .       -1.653441\n",
      "NUM    -> PRON    -1.728971\n",
      "NUM    -> DET     -1.928503\n",
      "PRT    -> CONJ    -2.047916\n",
      "DET    -> PRT     -2.612784\n",
      "CONJ   -> CONJ    -3.387113\n",
      "CONJ   -> X       -3.636751\n",
      "CONJ   -> .       -4.429760\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "17.220056 NUM      lowercase_word:million\n",
      "17.015641 DET      lowercase_word:these\n",
      "16.687589 VERB     lowercase_word:were\n",
      "16.099488 ADV      lowercase_word:n't\n",
      "14.830224 NUM      lowercase_word:billion\n",
      "14.680093 CONJ     lowercase_word:and\n",
      "14.476164 ADP      lowercase_word:during\n",
      "14.165553 DET      lowercase_word:which\n",
      "13.829237 DET      lowercase_word:each\n",
      "13.768926 ADJ      lowercase_word:willing\n",
      "13.684326 ADP      lowercase_word:than\n",
      "13.610947 NOUN     lowercase_word:something\n",
      "13.517312 PRON     lowercase_word:themselves\n",
      "13.385888 DET      lowercase_word:those\n",
      "13.368296 NOUN     lowercase_word:anything\n",
      "13.347438 ADV      lowercase_word:not\n",
      "13.317618 ADV      lowercase_word:o'clock\n",
      "13.243296 NUM      lowercase_word:hundred\n",
      "13.173118 NUM      lowercase_word:five\n",
      "13.043313 PRON     lowercase_word:hers\n",
      "12.956917 ADJ      lowercase_word:outstanding\n",
      "12.956593 ADV      lowercase_word:often\n",
      "12.760283 ADV      lowercase_word:sometimes\n",
      "12.703839 DET      lowercase_word:both\n",
      "12.654774 ADV      lowercase_word:instead\n",
      "12.535900 VERB     lowercase_word:been\n",
      "12.197426 NOUN     lowercase_word:nothing\n",
      "12.154810 CONJ     lowercase_word:or\n",
      "11.992988 PRON     lowercase_word:ourselves\n",
      "11.924560 ADP      lowercase_word:unless\n",
      "\n",
      "Top negative:\n",
      "-5.127945 ADP      next.word.lower:jim\n",
      "-5.128671 ADV      previous.word.lower:friends\n",
      "-5.204501 DET      next.word.lower:450\n",
      "-5.243220 NOUN     next.word.lower:southern\n",
      "-5.306742 ADV      next.word.lower:record\n",
      "-5.347656 NOUN     previous.word.lower:ones\n",
      "-5.406777 CONJ     previous.word.lower:choice\n",
      "-5.412712 DET      next.word.lower:2\n",
      "-5.414581 X        next.word.lower:*-1\n",
      "-5.451233 CONJ     previous.word.lower:help\n",
      "-5.463743 VERB     previous.word.lower:hard\n",
      "-5.466593 ADP      next.word.lower:engine\n",
      "-5.523541 NOUN     lowercase_word:well\n",
      "-5.547275 NOUN     next.word.lower:*u*\n",
      "-5.711153 VERB     next.word.lower:content\n",
      "-5.794245 DET      next.word.lower:*ich*-1\n",
      "-5.827128 ADJ      word[-2:]:is\n",
      "-5.856667 NOUN     previous.word.lower:shares\n",
      "-5.890721 CONJ     previous.word.lower:any\n",
      "-5.903066 DET      next.word.lower:practical\n",
      "-5.930749 VERB     lowercase_word:subject\n",
      "-6.216390 PRON     next.word.lower:things\n",
      "-6.246356 VERB     lowercase_word:down\n",
      "-6.565561 .        previous.word.lower:c.\n",
      "-6.795203 NOUN     lowercase_word:'s\n",
      "-6.948302 ADV      previous.word.lower:asia\n",
      "-7.063754 ADP      next.word.lower:down\n",
      "-7.353129 NOUN     previous.word.lower:employees\n",
      "-7.356359 DET      next.word.lower:ill\n",
      "-7.696527 ADP      next.word.lower:rear\n"
     ]
    }
   ],
   "source": [
    "# important features\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
