{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')], [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')], [('The', 'DET'), ('asbestos', 'NOUN'), ('fiber', 'NOUN'), (',', '.'), ('crocidolite', 'NOUN'), (',', '.'), ('is', 'VERB'), ('unusually', 'ADV'), ('resilient', 'ADJ'), ('once', 'ADP'), ('it', 'PRON'), ('enters', 'VERB'), ('the', 'DET'), ('lungs', 'NOUN'), (',', '.'), ('with', 'ADP'), ('even', 'ADV'), ('brief', 'ADJ'), ('exposures', 'NOUN'), ('to', 'PRT'), ('it', 'PRON'), ('causing', 'VERB'), ('symptoms', 'NOUN'), ('that', 'DET'), ('*T*-1', 'X'), ('show', 'VERB'), ('up', 'PRT'), ('decades', 'NOUN'), ('later', 'ADJ'), (',', '.'), ('researchers', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('*T*-2', 'X'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# first few tagged sentences\n",
    "print(nltk_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957\n",
      "1957\n",
      "[[('Year', 'NOUN'), ('ended', 'VERB'), ('Dec.', 'NOUN'), ('31', 'NUM'), (',', '.'), ('1988', 'NUM'), (':', '.'), ('Net', 'ADJ'), ('income', 'NOUN'), (':', '.'), ('$', '.'), ('65', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), (';', '.'), ('or', 'CONJ'), ('$', '.'), ('1.49', 'NUM'), ('*U*', 'X'), ('a', 'DET'), ('share', 'NOUN')], [('It', 'PRON'), ('hopes', 'VERB'), ('*-1', 'X'), ('to', 'PRT'), ('speak', 'VERB'), ('to', 'PRT'), ('students', 'NOUN'), ('at', 'ADP'), ('theological', 'ADJ'), ('colleges', 'NOUN'), ('about', 'ADP'), ('the', 'DET'), ('joys', 'NOUN'), ('of', 'ADP'), ('bell', 'NOUN'), ('ringing', 'NOUN'), ('and', 'CONJ'), ('will', 'VERB'), ('shortly', 'ADV'), ('publish', 'VERB'), ('a', 'DET'), ('booklet', 'NOUN'), ('for', 'ADP'), ('every', 'DET'), ('vicar', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('country', 'NOUN'), ('entitled', 'VERB'), (',', '.'), ('``', '.'), ('The', 'DET'), ('Bells', 'NOUN'), ('in', 'ADP'), ('Your', 'PRON'), ('Care', 'NOUN'), ('.', '.'), (\"''\", '.')], [('These', 'DET'), ('small', 'ADJ'), ('but', 'CONJ'), ('influential', 'ADJ'), ('floor', 'NOUN'), ('brokers', 'NOUN'), ('long', 'ADV'), ('have', 'VERB'), ('earned', 'VERB'), ('fat', 'ADJ'), ('returns', 'NOUN'), ('of', 'ADP'), ('30', 'NUM'), ('%', 'NOUN'), ('to', 'PRT'), ('40', 'NUM'), ('%', 'NOUN'), ('*U*', 'X'), ('a', 'DET'), ('year', 'NOUN'), ('on', 'ADP'), ('their', 'PRON'), ('capital', 'NOUN'), (',', '.'), ('by', 'ADP'), ('virtue', 'NOUN'), ('of', 'ADP'), ('their', 'PRON'), ('monopoly', 'NOUN'), ('in', 'ADP'), ('*', 'X'), ('making', 'VERB'), ('markets', 'NOUN'), ('in', 'ADP'), ('individual', 'ADJ'), ('stocks', 'NOUN'), ('.', '.')], [('He', 'PRON'), ('will', 'VERB'), ('be', 'VERB'), ('eligible', 'ADJ'), ('for', 'ADP'), ('an', 'DET'), ('annual', 'ADJ'), ('pension', 'NOUN'), ('of', 'ADP'), ('more', 'ADJ'), ('than', 'ADP'), ('$', '.'), ('244,000', 'NUM'), ('*U*', 'X'), ('with', 'ADP'), ('certain', 'ADJ'), ('other', 'ADJ'), ('fringe', 'NOUN'), ('benefits', 'NOUN'), ('.', '.')], [('Pepperdine', 'NOUN'), ('University', 'NOUN'), ('economist', 'NOUN'), ('Dean', 'NOUN'), ('Baim', 'NOUN'), ('scoffs', 'VERB'), ('at', 'ADP'), ('that', 'DET'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "random.seed(1234)\n",
    "train_set, val_set = train_test_split(nltk_data,test_size=0.5)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(train_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50218"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Their',\n",
       " 'report',\n",
       " 'appears',\n",
       " 'in',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'issue',\n",
       " 'of',\n",
       " 'the',\n",
       " 'journal']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8490\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "V = set(tokens)\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRON', '.', 'VERB', 'CONJ', 'ADJ', 'X', 'ADV', 'NUM', 'NOUN', 'DET', 'ADP', 'PRT'}\n"
     ]
    }
   ],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.78452647e-03,   4.12147492e-02,   4.83731031e-01,\n",
       "          4.33839485e-03,   7.66449720e-02,   9.61677507e-02,\n",
       "          3.90455537e-02,   5.06146066e-03,   2.03181490e-01,\n",
       "          7.95372389e-03,   2.45842375e-02,   1.22921187e-02],\n",
       "       [  7.18080550e-02,   9.64867175e-02,   8.91173929e-02,\n",
       "          6.06683791e-02,   4.30162810e-02,   2.53641810e-02,\n",
       "          4.83290479e-02,   7.67780617e-02,   2.22793490e-01,\n",
       "          1.72579259e-01,   9.08311903e-02,   2.05655536e-03],\n",
       "       [  3.61570232e-02,   3.68949249e-02,   1.68536007e-01,\n",
       "          6.34592678e-03,   6.19834699e-02,   2.14433298e-01,\n",
       "          8.23494717e-02,   2.00708378e-02,   1.11717828e-01,\n",
       "          1.36658803e-01,   9.45985839e-02,   3.02538369e-02],\n",
       "       [  6.04255311e-02,   3.48936170e-02,   1.59999996e-01,\n",
       "          0.00000000e+00,   1.07234046e-01,   5.95744699e-03,\n",
       "          5.36170229e-02,   4.76595759e-02,   3.60851049e-01,\n",
       "          1.08936168e-01,   5.36170229e-02,   6.80851052e-03],\n",
       "       [  0.00000000e+00,   6.68130517e-02,   1.28607275e-02,\n",
       "          1.53701380e-02,   6.90087825e-02,   2.03889590e-02,\n",
       "          5.95984934e-03,   2.03889590e-02,   6.93538249e-01,\n",
       "          5.33249695e-03,   7.84190744e-02,   1.19196987e-02],\n",
       "       [  5.48536219e-02,   1.60246536e-01,   2.07396001e-01,\n",
       "          1.01694912e-02,   1.66409854e-02,   7.45762736e-02,\n",
       "          2.95839757e-02,   2.46533123e-03,   6.74884468e-02,\n",
       "          5.48536219e-02,   1.37750387e-01,   1.83975354e-01],\n",
       "       [  1.39064472e-02,   1.32111251e-01,   3.52085978e-01,\n",
       "          6.32111263e-03,   1.34007588e-01,   2.21238937e-02,\n",
       "          8.65992382e-02,   2.78128944e-02,   2.97092292e-02,\n",
       "          6.57395720e-02,   1.16308473e-01,   1.32743362e-02],\n",
       "       [  0.00000000e+00,   1.12312309e-01,   2.34234240e-02,\n",
       "          1.68168172e-02,   3.06306314e-02,   2.17417419e-01,\n",
       "          3.60360369e-03,   1.78378373e-01,   3.53153139e-01,\n",
       "          3.60360369e-03,   3.84384394e-02,   2.22222228e-02],\n",
       "       [  4.22028499e-03,   2.38964990e-01,   1.44873396e-01,\n",
       "          4.45551388e-02,   1.28684100e-02,   2.73972601e-02,\n",
       "          1.72270648e-02,   9.27079003e-03,   2.65670389e-01,\n",
       "          1.48055898e-02,   1.77321151e-01,   4.28255163e-02],\n",
       "       [  3.39136343e-03,   1.89916342e-02,   4.25050855e-02,\n",
       "          6.78272685e-04,   2.06194893e-01,   4.65747230e-02,\n",
       "          1.15306349e-02,   1.92177258e-02,   6.35993659e-01,\n",
       "          5.65227214e-03,   9.26972646e-03,   0.00000000e+00],\n",
       "       [  6.73564598e-02,   3.99675407e-02,   8.11523665e-03,\n",
       "          6.08642702e-04,   1.04686551e-01,   3.61128002e-02,\n",
       "          1.07526882e-02,   6.24873191e-02,   3.19334537e-01,\n",
       "          3.32521796e-01,   1.66362338e-02,   1.42016634e-03],\n",
       "       [  1.98336523e-02,   3.83877158e-02,   3.99872035e-01,\n",
       "          1.27959053e-03,   8.57325643e-02,   1.27959056e-02,\n",
       "          8.95713363e-03,   4.92642373e-02,   2.55918115e-01,\n",
       "          1.07485607e-01,   1.91938579e-02,   1.27959053e-03]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tagged_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-59fe24e1431b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Viterbi Heuristic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mViterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_bag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_tagged_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_bag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_tagged_words' is not defined"
     ]
    }
   ],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
